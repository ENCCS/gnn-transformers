{"cells":[{"attachments":{},"cell_type":"markdown","metadata":{"id":"VZn9DE1fyxQO"},"source":["# Summing or concatenating embeddings?\n","\n","Online version [here](http://colab.research.google.com/github/ENCCS/gnn_transformers_notebooks/blob/main/colab/1b_vector_sums_vs_concatenation.ipynb)\n","\n","In neural networks, there are often cases where you might have two or more vectors from different sources, e.g. outputs of parallel networks like in the multihead attention of the Transformer or multiple embeddings from multiple categorical variables as inputs.\n","\n","One common way of dealing with this is to _concatenate_ the multiple vectors to a single result. Let's say we have two column vectors $\\mathbf{u}$ and $\\mathbf{v}$, then we think of the concatenation as:\n","\n","$$ \\mathbf{x} = \\begin{bmatrix} \\mathbf{u} \\\\ \\mathbf{v} \\end{bmatrix}$$\n","\n","\n","What we'll argue in this notebook is that a neater way of combining the vectors is by instead using a sum:\n","\n","$$ \\mathbf{x} = \\mathbf{u} + \\mathbf{v}$$\n","\n","For _embeddings_ in particular we'll make a strong argument for this.\n"]},{"cell_type":"markdown","metadata":{"id":"EQA7GWMJhx1w"},"source":["## Why is summing embeddings better than concatenation for aggregating vectors?\n","\n","First we rely on these assumptions:\n"," - The vectors to sum consists of _free parameters_ or are some result from a computation including free parameters, they are free to take any value by our optimzation procedure.\n"," - We will apply a linear transformation to the result of the aggregation\n"," - the dimensionality of the aggregation result is the same for both aggregation methods\n","\n","\n","### Problem with concatenation - decide on dimensionality\n","First, consider aggregating by concatenation\n","\n"," $$ \\mathbf{x} = \\begin{bmatrix} \\mathbf{u} \\\\ \\mathbf{v} \\end{bmatrix}$$\n","\n","\n"," Now let $\\mathbf{x} \\in \\mathord{R}^d$, then $\\mathbf{u} \\in \\mathord{R}^k, k < d$ and $\\mathbf{u} \\in \\mathord{R}^{d-k}$. \n","\n","In other words, we have to select the dimensionality of $\\mathbf{u}$ and $\\mathbf{v}$ so that it adds up to the desired $d$. How do we do this? Which of the vectors $\\mathbf{u}$ and $\\mathbf{v}$ _needs_ the most capacity?\n","\n","If we instead use sums: \n","\n","$$ \\mathbf{x} = \\mathbf{u} + \\mathbf{v}$$\n","\n","It's easy to see that we just have the same dimensionality: $\\mathbf{x}, \\mathbf{u},  \\mathbf{v} \\in \\mathord{R}^d$\n","\n","### Implementing concatenation using a sum\n","One argument for why sums are at least as exprissive as concatenation is to see that we can actually implement concatenation using sum. Let's say we have decided on some dimensionality of our $\\mathbf{u}$ and $\\mathbf{v}$. We now have that  $\\mathbf{u}$ and $\\mathbf{v}$ are\n","\n","$$\n","\\mathbf{u} = \\begin{bmatrix} \n","u_1\\\\\n","\\vdots\\\\\n","u_k\n","\\end{bmatrix}\n","\\mathbf{v} = \\begin{bmatrix} \n","v_1\\\\\n","\\vdots\\\\\n","v_{d-k}\n","\\end{bmatrix}$$\n","\n","and\n","\n","$$ \n","\\mathbf{x} = \\begin{bmatrix} \\mathbf{u} \\\\ \\mathbf{v} \\end{bmatrix} = \n"," \\begin{bmatrix} \n","u_1\\\\\n","\\vdots\\\\\n","u_k\\\\\n","v_1\\\\\n","\\vdots\\\\\n","v_{d-k}\n","\\end{bmatrix}$$\n","\n","Now if we instead construct $0$-padded version of $\\mathbf{u}$ and $\\mathbf{v}$, $\\mathbf{u}'$ and $\\mathbf{v}'$\n","\n","\n","$$\n","\\mathbf{u}' = \\begin{bmatrix} \n","u_1\\\\\n","\\vdots\\\\\n","u_k\\\\\n","0\\\\\n","\\vdots\\\\\n","0\n","\\end{bmatrix}\n","\\mathbf{v}' = \\begin{bmatrix} \n","0\\\\\n","\\vdots\\\\\n","0\\\\\n","v_1\\\\\n","\\vdots\\\\\n","v_{d-k}\n","\\end{bmatrix}$$\n","\n","We can implement the concatenation using this sum\n","\n","$$\\mathbf{x} = \\begin{bmatrix} \\mathbf{u} \\\\ \\mathbf{v} \\end{bmatrix} = \n","\\mathbf{u}' + \\mathbf{v}'\n","$$\n","\n","So you see that concatenation can be seen as a special case of a sum by constructing the vectors to sum in a specific manner. \n","\n","In general though, we would not set parts of the vectors to sum to hard zeros, instead having them filled with values which the optimization procedure is free to use however is appropriate. This is a difference, the concatenation explicitly separates the subspaces for the vectors, which means that _learning_ dynamics might behave differently. \n","\n","An analogy is to that of fully connected neural networks vs. convolutional. A convolutional layer can be seen as a special case of a fully connected layers, where many of the weights are hard zeroes and many of the other weights are tied. This constraint actually makes CNNs much more data efficient, in that they exploit prior beliefs about how to analyze spatially correlated date. \n","\n","This bias of concatenation to behave as sums of vectors in orthogonal subspaces can be useful when you _do_ want to concatenate vectors but your framework might be limited (e.g. concatenating multiple tensors in a single operation).\n","\n"]},{"cell_type":"markdown","metadata":{"id":"yof5vAY3oj54"},"source":["## Concatenation and sums when followed by a linear transformation\n","\n","There's a better argument for summing instead of concatenating if we also assume that this aggregation will be multiplied with a matrix (as is often the case in neural networks).\n","\n","In that case, lets say we concatenate the vectors of _free variables_ $\\mathbf{u}$ and $\\mathbf{v}$:\n","\n","$$\\mathbf{x} = \\begin{bmatrix} \\mathbf{u} \\\\ \\mathbf{v} \\end{bmatrix}$$\n","\n","Then the matrix multiplication $W \\mathbf{x}$ is the same as \n","$$\n","W \\mathbf{x} = W_u \\mathbf{u} + W_v \\mathbf{v}\n","$$\n","\n","Where $W = \\begin{bmatrix} W_u & W_v \\end{bmatrix}$\n","\n","If we use concatenation, the matrix-vector multiplication that the concatenation encounters will essentially perform a _sum_ of the concatenated vectors _after_ they have been linearily transformed. \n","\n","Since all the matrices and vectors are _freely parameterized_, we might just set $W_u = W_v$ and not loose anything in terms of expressivity, especially if we increase the dimensionality of $\\mathbf{u}$ and $\\mathbf{v}$ to match the one the concatenation would have had.\n","\n","Another way to think about this is to consider where $\\mathbf{u}$ and $\\mathbf{v}$ come from. Let's say that they are actually the $i$th and $j$th column of two _embedding_ matrices $E_u$ and $E_v$ (using numpy slice notation):\n","\n","$$\\mathbf{u} = E_u[:,i]$$\n","$$\\mathbf{v} = E_u[:,j]$$\n","\n","Then \n","\n","$$\n","W \\mathbf{x} = W_u \\mathbf{u} + W_v \\mathbf{v} =  W_u E_u[:,i] + W_v E_u[:,j] \n","$$\n","\n","Since these operations are associative, that is $W (E[:,i]) = (W E)[:, i]$, we might as well just set our embeddings to \n","\n","\n","$$E_u' = (W_u E_u), E_v' = (W_v E_v)$$ \n","\n","\n","and use sum to achieve the exact same thing as concatenation would have achieved. Since these are all just free parameters (typically randomly initialized), the only difference between $E$ and $E'$ is the random distribution the values are drawn from.\n","\n","##An argument **for** concatenation \n","\n","If we would like our combination of vectors to _not_ be permutation invariant, we _might_ want to use concatenation. Let's say the vectors we want to combine are actually a sequence, where each position in the sequence will be represented by a vector, but this might be the _same_ vector.\n","\n","$S_1 = \\text{\"a white cat visited the house\"}$\n","\n","$S_2 = \\text{\"a cat visited the white house\"}$\n","\n","In this case, concatenation makes more sense\n","\n","$$S_1 = \\begin{bmatrix} \\mathbf{x}_\\text{a} \\\\ \\mathbf{x}_\\text{white} \\\\ \\mathbf{x}_\\text{cat} \\\\ \\mathbf{x}_\\text{visited} \\\\ \\mathbf{x}_\\text{the} \\\\  \n","\\mathbf{x}_\\text{house} \\\\  \n","\\end{bmatrix}$$\n","\n","$$S_2 = \\begin{bmatrix} \\mathbf{x}_\\text{a} \\\\ \\mathbf{x}_\\text{cat} \\\\ \\mathbf{x}_\\text{visited} \\\\ \\mathbf{x}_\\text{the} \\\\ \\mathbf{x}_\\text{white} \\\\ \n","\\mathbf{x}_\\text{house} \n","\\end{bmatrix}$$\n","\n","\n","$$\n","W S_1 = W_1 \\mathbf{x}_\\text{a} + W_2\\mathbf{x}_\\text{white} + W_3\\mathbf{x}_\\text{cat} + W_4 \\mathbf{x}_\\text{visited} + W_5\\mathbf{x}_\\text{the} +  W_6\\mathbf{x}_\\text{house} \n","$$\n","\n","$$\n","W S_2 = W_1 \\mathbf{x}_\\text{a} + W_2\\mathbf{x}_\\text{cat} + W_3\\mathbf{x}_\\text{visited} + W_4 \\mathbf{x}_\\text{the} + W_5\\mathbf{x}_\\text{white} +  W_6\\mathbf{x}_\\text{house} \n","$$\n","\n","So in this case, assuming that the $\\mathbf{x}$'s are fixed, the model can learn to _project_ them differently depending on _position_ in the input concatenation. The word vector $W_5\\mathbf{x}_\\text{white}$ will be different from $W_2\\mathbf{x}_\\text{white}$, even though it's the same word.\n","\n","If we instead summed them, we could not separate these two sentence representations.\n","\n","Conversely, if we do want permutation invariance, concatenation is problematic since\n","\n","$$\n"," W_1 \\mathbf{x}_1 + W_2 \\mathbf{x}_2 \\neq W_1 \\mathbf{x}_2 + W_2 \\mathbf{x}_2\n","$$\n","\n","So if the combination _should_ be permutation invariant, this will cause issues (the network can still _learn_ to treat them permutation invariant learning similar  $W_1$ and $W_2$, but this will require much more training).\n","\n","We could _still_ achieve the same thing using sums by having different embeddings based on position, so $\\mathbf{x}_\\text{2, white}$ \n","will be different from $\\mathbf{x}_\\text{5, white}$\n","\n","In practice, this would mean we create separate Embeddings for each position."]},{"cell_type":"markdown","metadata":{"id":"gGA00ZBn08Pt"},"source":["## Sums of vectors in high dimensions\n","Intuitively, summing vectors _feels_ wrong, since we're essentially making information hard to disentangle. Let's say we have som set of numbers $X = \\{x_1, ..., x_n\\}$ and we form some value $y = x_i + x_j$. It's difficult to determine exactly which numbers went into this sum, especially if the sum could have any number of terms. This intuition also tells us that summing vectors (so $X = \\{ \\mathbf{x}_1, ... \\mathbf{x}_n\\}$) should be bad, wont we do the same thing as before?\n","\n","We'll make some experiments with sums of vectors and see how destructive this operation is"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"e3zcbDPQ0I9w"},"outputs":[],"source":["import numpy as np\n","from collections import Counter, defaultdict\n","import matplotlib.pyplot as plt\n","\n","\n","np.random.seed(1729)\n","\n","num_values = 32\n","sample_sizes = [10, 50, 100, 500]\n","\n","raw_data_samples = {}\n","for sample_size in sample_sizes:\n","  raw_data_sample = np.random.choice(num_values, size=sample_size)\n","  raw_data_samples[sample_size] = raw_data_sample"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bZmnuach0rA1"},"outputs":[],"source":["embedding_dims = [2, 16, 24, 32, 64, 128]\n","embeddings_by_dim = {}\n","num_embeddings = num_values\n","for embedding_dim in embedding_dims:\n","  # We want to make embeddings have the same values in overlapping dimensions\n","  # We reset the random seed to the same value to achieve this\n","  # Numpy assumes c-contiguous order, so we need to generate the embeddings in\n","  # transpose, and then transpose them back\n","  np.random.seed(1729)  \n","  # This is the transpose of how we would typically have the embeddings laied out\n","  embeddings_T = np.random.randn(embedding_dim, num_embeddings)  \n","  embeddings = embeddings_T.T  # Transpose them so they are in the assumed layout\n","  embeddings_by_dim[embedding_dim] = embeddings"]},{"cell_type":"markdown","metadata":{"id":"A21r6ME_RINr"},"source":["Now let's creat our summed embeddings for the different sample sizes and embedding dimensionalities"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Nuk62w0D2aDS"},"outputs":[],"source":["embedded_samples = {}\n","for sample_size, raw_data_sample in raw_data_samples.items():\n","  for embedding_dim, embeddings in embeddings_by_dim.items():\n","    embedded_sample = embeddings[raw_data_sample]\n","    summed_vectors = embedded_sample.sum(axis=0)\n","    embedded_samples[(sample_size, embedding_dim)] = summed_vectors\n"]},{"cell_type":"markdown","metadata":{"id":"O076UlmTsg4X"},"source":["We have now created sums of vectors for different number of vectors and different embedding dimensions. Let's see if we can figure out what embedding vectors, and how many of them where actually part of these sums.\n","\n","We use a simple iterative algorithm which compares the sum using cosine similarity (dot product between normalized vectors) to at each iteration pick the vector with the highest cosine similarity to the sum at retracting that vector from the sum."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"X4Rpi-E2WZdD"},"outputs":[],"source":["def estimate_vector_counts(vector_sum, embeddings, n):\n","  '''Try to figure out what vectors from *embeddings* was part of the \n","  *vector_sum* of *n* vectors'''\n","  # We'll use the cosine similarity to select the top vector at each iteration\n","  embeddings_normalized = embeddings / np.linalg.norm(embeddings, \n","                                                  axis=1, \n","                                                  keepdims=True)\n","  x = vector_sum[:]\n","  counts = Counter()\n","  for i in range(n):\n","    x_normalized = x / np.linalg.norm(x)\n","    cosine_sim = embeddings_normalized @ x_normalized\n","    top_emb_idx = np.argmax(cosine_sim)\n","    top_emb_vec = embeddings[top_emb_idx]\n","    x = x - top_emb_vec\n","    counts[top_emb_idx] += 1\n","  return counts\n"]},{"cell_type":"markdown","metadata":{"id":"A05jl_hps7R_"},"source":["Below we illustrate how well we can pick out vectors using this simple method. The histograms at each subfigure shows us the true count of the different vectors in blue, and the estimated count in orange.\n","\n","As you can see, when the embedding dimension increases, the ability to pick out the vector sum improves dramatically. This isn't really suprising. If we had one-hot vectors representing the data, we could easily do the same by just looking at the count at each position, but you can see that we're still getting decent results with an embedding dimension which is less than the number of vectors even though the embedding are random."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":855},"executionInfo":{"elapsed":8355,"status":"ok","timestamp":1637587630491,"user":{"displayName":"Erik Ylipää","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghv2ZnBVJIOqdwt_HBhCMHTtEbc2FUGzMgmKQEW7VE=s64","userId":"10747062110949017925"},"user_tz":-60},"id":"rHiqid87WTue","outputId":"6ef69608-beda-4ca0-d8c3-7d8f179b9765"},"outputs":[],"source":["fig, axes = plt.subplots(nrows=len(sample_sizes), ncols=len(embedding_dims), figsize=(16,10), sharex='all', sharey='none', dpi=90)\n","\n","legend = None\n","\n","for i, sample_size in enumerate(sample_sizes):\n","  for j, embedding_dim in enumerate(embedding_dims):\n","    true_sample_count = Counter(raw_data_samples[sample_size])\n","    embeddings = embeddings_by_dim[embedding_dim]\n","    vector_sum = embedded_samples[(sample_size, embedding_dim)]\n","    estimated_counts = estimate_vector_counts(vector_sum, embeddings, sample_size)\n","    true_x, true_y = zip(*sorted(true_sample_count.items()))\n","    est_x, est_y = zip(*sorted(estimated_counts.items()))\n","    ax = axes[i,j]\n","    width = 0.35\n","    artist_1 = ax.bar(np.array(true_x) - width/2, true_y, width, label=\"Actual vector indices\")\n","    artist_2 = ax.bar(np.array(est_x) + width/2, est_y, width, label=\"Estimated vector indices\")\n","    if legend is None:\n","      legend = fig.legend()\n","    if i == 0:\n","      ax.set_title(f'Embedding dim {embedding_dim}')\n","    if j == 0:\n","      ax.text(-0.5, 0.5, f'Number of vectors {sample_size}', rotation=90,  transform=ax.transAxes, ha='center', va='center')\n","      ax.set_ylabel('Count')\n","    if i == len(sample_sizes)-1:\n","      ax.set_xlabel('Vector index')\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":338,"status":"ok","timestamp":1637581121652,"user":{"displayName":"Erik Ylipää","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghv2ZnBVJIOqdwt_HBhCMHTtEbc2FUGzMgmKQEW7VE=s64","userId":"10747062110949017925"},"user_tz":-60},"id":"2f6bjD2cWU2M","outputId":"e585128c-2c52-46a1-e091-3a015a562bae"},"outputs":[],"source":["Counter([1,2,4,5,1,2]) - Counter([1,2,5,7])"]},{"cell_type":"markdown","metadata":{"id":"lEGHDUUWuNgc"},"source":["We can simplify the visualization of the data above, and tally the miscounts. This will allow us to get a better idea of how embedding dimension, number of embeddings vectors (vocabulary) and number of vectors in the sum relate to each other."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QOAIV1HgcHRK"},"outputs":[],"source":["def get_miscounts(true_count, estimated_count):\n","  counts = dict(true_count.items())\n","  for x, est_count in estimated_count.items():\n","    if x in counts:\n","      counts[x] = counts[x] - est_count\n","    else:\n","      counts[x] = -est_count\n","  return sum([abs(x) for x in counts.values()])\n"]},{"cell_type":"markdown","metadata":{"id":"QdPIJXpkzFSe"},"source":["Here we look at combinations of vocabulary size (number of distinct embedding vectors), embedding dimension and sample size (number of vectors in the sum)."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hr5h0XT8cmTP"},"outputs":[],"source":["import numpy as np\n","from collections import Counter, defaultdict\n","import matplotlib.pyplot as plt\n","\n","miscounts_num_distinct_vectors = defaultdict(lambda: defaultdict(dict))\n","\n","\n","num_distinct_vectors = [8, 32, 128, 1024]\n","embedding_dims = [10, 20, 50, 70] + [x*100 for x in range(1,11)]\n","sample_sizes = [10, 20, 50, 100, 200]\n","\n","for num_vectors in num_distinct_vectors:\n","  np.random.seed(1729)\n","  raw_data_samples = {}\n","  for sample_size in sample_sizes:\n","    raw_data_sample = np.random.choice(num_vectors, size=sample_size)\n","    raw_data_samples[sample_size] = raw_data_sample\n","  embeddings_by_dim = {}\n","  num_embeddings = num_vectors\n","  for embedding_dim in embedding_dims:\n","    # We want to make embeddings have the same values in overlapping dimensions\n","    # We reset the random seed to the same value to achieve this\n","    # Numpy assumes c-contiguous order, so we need to generate the embeddings in\n","    # transpose, and then transpose them back\n","    np.random.seed(1729)  \n","    # This is the transpose of how we would typically have the embeddings laied out\n","    embeddings_T = np.random.randn(embedding_dim, num_embeddings)  \n","    embeddings = embeddings_T.T  # Transpose them so they are in the assumed layout\n","    embeddings_by_dim[embedding_dim] = embeddings\n","  embedded_samples = {}\n","  for sample_size, raw_data_sample in raw_data_samples.items():\n","    for embedding_dim, embeddings in embeddings_by_dim.items():\n","      embedded_sample = embeddings[raw_data_sample]\n","      summed_vectors = embedded_sample.sum(axis=0)\n","      embedded_samples[(sample_size, embedding_dim)] = summed_vectors\n","\n","  for i, sample_size in enumerate(sample_sizes):\n","    for j, embedding_dim in enumerate(embedding_dims):\n","      true_sample_count = Counter(raw_data_samples[sample_size])\n","      embeddings = embeddings_by_dim[embedding_dim]\n","      vector_sum = embedded_samples[(sample_size, embedding_dim)]\n","      estimated_counts = estimate_vector_counts(vector_sum, embeddings, sample_size)\n","      miscount = get_miscounts(true_sample_count, estimated_counts)\n","      miscounts_num_distinct_vectors[num_vectors][sample_size][embedding_dim] = miscount"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"elapsed":1867,"status":"ok","timestamp":1637587156750,"user":{"displayName":"Erik Ylipää","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghv2ZnBVJIOqdwt_HBhCMHTtEbc2FUGzMgmKQEW7VE=s64","userId":"10747062110949017925"},"user_tz":-60},"id":"vhIuETWSv2jK","outputId":"08c2b562-8e9d-45de-e424-cda14debf36d"},"outputs":[],"source":["fig, axes = plt.subplots(nrows=2, ncols=2, figsize=(12,10), dpi=150)\n","\n","for num_vectors, ax in zip(num_distinct_vectors, axes.flatten()):\n","  miscounts_by_sample_size = miscounts_num_distinct_vectors[num_vectors]\n","  for sample_size, miscounts_by_emb_dim in miscounts_by_sample_size.items():\n","    emb_dims, miscount = zip(*sorted(miscounts_by_emb_dim.items()))\n","    ax.plot(emb_dims, miscount, label=f\"Sample size {sample_size}\")\n","  ax.set_title(f'Vocabulary size {num_vectors}')\n","  ax.legend()\n","  ax.set_xlabel('Embedding dimension')\n","  ax.set_ylabel('Miscounts')\n","  "]},{"cell_type":"markdown","metadata":{"id":"cVaSEensx-TA"},"source":["As you can see, with sums of random vectors we can with quite good accuracy pick out exactly how many of each vector was part of the sum. Since a neural network relies on very similar mechanisms (dot product between weight vector and inputs), they also have this ability to pick out exactly what vector was part of a sum, provided the dimensionality is high enough.\n","\n","In the case of neural networks, we often opimize the embedding vectors, so the results above should be thought of as a worst case. Our optimization can learn to set the embedding vectors so they perform as well as possible for the desired task."]},{"cell_type":"markdown","metadata":{"id":"1CBiGwkg49RH"},"source":["## When you shouldn't sum vectors\n","\n","In the examples we've shown above, the arguments for summing over concatenating comes from the assumption that the vectors are _high dimensional_ and contain _free parameters_, we can set their values to anything. This is why we can say something like _\"we could just replace the vector matrix $W_u \\mathbf{u}$ with $\\mathbf{u}'$\"_. If they are free, say $\\mathbf{u}$ is actually a vector of measurements, then the arguments will not hold, sum and concatenation followed by a matrix multiplication will not be equivalent in terms of expressivity.\n","\n","Also, if you have a set of scalars you would like to use as inputs, you can't sum them toghether and hope it will work (if they are in very different ranges, the nerual network might learn to disentangle this kind of sum, but we often standardize numerical variables so don't rely on this).\n","\n","Likewise, if you have two vectors of different concatenated scalar _measurements_, summing them together followed by a matrix multiplication _will not_ be the same as concatenating them."]}],"metadata":{"colab":{"authorship_tag":"ABX9TyOBckbdCs31RMo9J0s38nnj","collapsed_sections":[],"name":"1b_vector_sums_vs_concatenation.ipynb","provenance":[],"toc_visible":true},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}
