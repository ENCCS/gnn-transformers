{"nbformat": 4, "nbformat_minor": 0, "metadata": {"colab": {"name": "4a_GNNs_to_Transformers.ipynb", "provenance": [], "collapsed_sections": ["3giz2t6yy0y6"], "toc_visible": true, "authorship_tag": "ABX9TyPA3Nn3obddzKGwjYcnyuOR"}, "kernelspec": {"name": "python3", "display_name": "Python 3"}, "language_info": {"name": "python"}, "accelerator": "GPU"}, "cells": [{"cell_type": "markdown", "source": ["# GNNs to Transformers\n", "\n", "We've previously seen how we can implement _graph convolutions_ using multiplication with the adjacency matrix. In this notebook we will look at how this can be extended to a _Transformer_ by  _computing_ an adjacency matrix.\n", "\n", "You can find this notebook on Google Colab [here](https://drive.google.com/file/d/1-lxSfJOdKbbHbVS0LHedb6rznf0D9m-K/view?usp=sharing)"], "metadata": {"id": "2A3c0f7A03R3"}}, {"cell_type": "markdown", "metadata": {"id": "XE8-UiVLebZo"}, "source": ["## Preamble\n", "This section downloads and install a conda environment on your colab virtual machine, which makes dealing with dependencies a lot easier"]}, {"cell_type": "markdown", "metadata": {"id": "KvohFD_uenKH"}, "source": ["### Installing condacolab\n", "We will use a package called condacolab which will download an initialize a conda environment which we can later install packages to"]}, {"cell_type": "code", "metadata": {"id": "h77ZMyN_wWaO", "colab": {"base_uri": "https://localhost:8080/"}, "executionInfo": {"status": "ok", "timestamp": 1637726255533, "user_tz": -60, "elapsed": 35629, "user": {"displayName": "Erik Ylip\u00e4\u00e4", "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Ghv2ZnBVJIOqdwt_HBhCMHTtEbc2FUGzMgmKQEW7VE=s64", "userId": "10747062110949017925"}}, "outputId": "342c2c22-f325-4c4b-cd09-0a6d82048385"}, "source": ["#Preamble, this will install a conda (mamba actually) environment on your Colab VM, which really makes working with complex dependencies (RDKit in this case) easier.\n", "# YOU WILL LIKELY GET A NOTIFICATION ABOUT YOUR SESSION CRASHING, THIS IS EXPECTED BEHAVIOUR (the install will restart the python kernel). \n", "# Wait until this cell is done before running the rest of the notebook.\n", "!pip install -q condacolab\n", "import condacolab\n", "condacolab.install()"], "execution_count": null, "outputs": [{"output_type": "stream", "name": "stdout", "text": ["\u23ec Downloading https://github.com/jaimergp/miniforge/releases/latest/download/Mambaforge-colab-Linux-x86_64.sh...\n", "\ud83d\udce6 Installing...\n", "\ud83d\udccc Adjusting configuration...\n", "\ud83e\ude79 Patching environment...\n", "\u23f2 Done in 0:00:30\n", "\ud83d\udd01 Restarting kernel...\n"]}]}, {"cell_type": "code", "metadata": {"colab": {"base_uri": "https://localhost:8080/"}, "id": "AZr98jGBmmaC", "executionInfo": {"status": "ok", "timestamp": 1637726263320, "user_tz": -60, "elapsed": 350, "user": {"displayName": "Erik Ylip\u00e4\u00e4", "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Ghv2ZnBVJIOqdwt_HBhCMHTtEbc2FUGzMgmKQEW7VE=s64", "userId": "10747062110949017925"}}, "outputId": "82e30ecb-73a3-49e6-fd0c-694d37d13e39"}, "source": ["# Check that we now have a working conda environment. You should get the output \"Everything looks OK!\"\n", "import condacolab\n", "condacolab.check()"], "execution_count": null, "outputs": [{"output_type": "stream", "name": "stdout", "text": ["\u2728\ud83c\udf70\u2728 Everything looks OK!\n"]}]}, {"cell_type": "markdown", "metadata": {"id": "N9ddFxnYezCN"}, "source": ["### Installing the notebook packages\n", "Now that condacolab is up and running, we install the packages. To install the correct version of cuda toolkits  for pytorch, we first check which version of cuda is installed on the VM. If the last line is not `cuda_11.1.xxx`, you need to change the mamba install line to match the version you see from `nvcc`"]}, {"cell_type": "code", "metadata": {"id": "VIOHL4ZagFLq", "colab": {"base_uri": "https://localhost:8080/"}, "executionInfo": {"status": "ok", "timestamp": 1637726266294, "user_tz": -60, "elapsed": 254, "user": {"displayName": "Erik Ylip\u00e4\u00e4", "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Ghv2ZnBVJIOqdwt_HBhCMHTtEbc2FUGzMgmKQEW7VE=s64", "userId": "10747062110949017925"}}, "outputId": "9a82855b-cdbd-4a1d-d62e-015da7c3dc0c"}, "source": ["!nvcc --version  # Check what cuda version is installed, this must match the cudatoolkit=XY.Z we give to the mamba install line"], "execution_count": null, "outputs": [{"output_type": "stream", "name": "stdout", "text": ["nvcc: NVIDIA (R) Cuda compiler driver\n", "Copyright (c) 2005-2020 NVIDIA Corporation\n", "Built on Mon_Oct_12_20:09:46_PDT_2020\n", "Cuda compilation tools, release 11.1, V11.1.105\n", "Build cuda_11.1.TC455_06.29190527_0\n"]}]}, {"cell_type": "code", "metadata": {"id": "CyHApbORm8eS"}, "source": ["# Install the required packages. Note that condacolab uses mamba (a conda \n", "# reimplementation) by default. This will likely take about 10 minutes\n", "!mamba install pytorch cudatoolkit=11.1 rdkit -c pytorch -c conda-forge > /dev/null"], "execution_count": null, "outputs": []}, {"cell_type": "markdown", "metadata": {"id": "3giz2t6yy0y6"}, "source": ["## Code from previous notebooks\n", "We will use the same example as in the previous notebooks."]}, {"cell_type": "code", "metadata": {"id": "h2QH_wF5g3nb"}, "source": ["from collections import defaultdict\n", "from collections.abc import Set\n", "\n", "import rdkit\n", "from rdkit.Chem import MolFromSmiles\n", "from rdkit.Chem.Draw import IPythonConsole\n", "from rdkit.Chem import Draw\n", "IPythonConsole.ipython_useSVG=True  #< set this to False if you want PNGs instead of SVGs\n", "IPythonConsole.drawOptions.addAtomIndices = True  # This will help when looking at the Mol graph representation\n", "IPythonConsole.molSize = 600, 600\n", "\n", "# We supress RDKit errors for this notebook\n", "from rdkit import RDLogger\n", "RDLogger.DisableLog('rdApp.*')  \n", "\n", "\n", "import torch\n", "from torch.utils.data import Dataset, DataLoader\n", "\n", "float_type = torch.float32  # We're hardcoding types in the tensors further down\n", "categorical_type = torch.long\n", "mask_type = torch.float32  # We're going to be multiplying our internal calculations with a mask using this type\n", "labels_type = torch.float32 # We're going to use BCEWithLogitsLoss, which expects the labels to be of the same type as the predictions"], "execution_count": null, "outputs": []}, {"cell_type": "code", "metadata": {"id": "bVhtCXz2Lpbb"}, "source": ["class ContinuousVariable:\n", "  def __init__(self, name):\n", "    self.name = name\n", "\n", "  def __repr__(self):\n", "    return f'<ContinuousVariable: {self.name}>'\n", "\n", "  def __eq__(self, other):\n", "    return self.name == other.name\n", "\n", "  def __hash__(self):\n", "    return hash(self.name)\n", "\n", "class CategoricalVariable:\n", "  def __init__(self, name, values, add_null_value=True):\n", "    self.name = name\n", "    self.has_null_value = add_null_value\n", "    if self.has_null_value:\n", "      self.null_value = None\n", "      values = (None,) + tuple(values)\n", "    self.values = tuple(values)\n", "    self.value_to_idx_mapping = {v: i for i, v in enumerate(values)}\n", "    self.inv_value_to_idx_mapping = {i: v for v, i in self.value_to_idx_mapping.items()}\n", "    \n", "    if self.has_null_value:\n", "      self.null_value_idx = self.value_to_idx_mapping[self.null_value]\n", "  \n", "  def get_null_idx(self):\n", "    if self.has_null_value:\n", "      return self.null_value_idx\n", "    else:\n", "      raise RuntimeError(f\"Categorical variable {self.name} has no null value\")\n", "\n", "  def value_to_idx(self, value):\n", "    return self.value_to_idx_mapping[value]\n", "  \n", "  def idx_to_value(self, idx):\n", "    return self.inv_value_to_idx_mapping[idx]\n", "  \n", "  def __len__(self):\n", "    return len(self.values)\n", "  \n", "  def __repr__(self):\n", "    return f'<CategoricalVariable: {self.name}>'\n", "\n", "  def __eq__(self, other):\n", "    return self.name == other.name and self.values == other.values\n", "\n", "  def __hash__(self):\n", "    return hash((self.name, self.values))"], "execution_count": null, "outputs": []}, {"cell_type": "code", "metadata": {"id": "4CIiwUfjTjua"}, "source": ["ATOM_SYMBOLS = ['H', 'He', 'Li', 'Be', 'B', 'C', 'N', 'O', 'F', 'Ne', 'Na', \n", "                'Mg', 'Al', 'Si', 'P', 'S', 'Cl', 'Ar', 'K', 'Ca', 'Sc', 'Ti', \n", "                'V', 'Cr', 'Mn', 'Fe', 'Co', 'Ni', 'Cu', 'Zn', 'Ga', 'Ge', 'As', \n", "                'Se', 'Br', 'Kr', 'Rb', 'Sr', 'Y', 'Zr', 'Nb', 'Mo', 'Tc', 'Ru', \n", "                'Rh', 'Pd', 'Ag', 'Cd', 'In', 'Sn', 'Sb', 'Te', 'I', 'Xe', 'Cs', \n", "                'Ba', 'Hf', 'Ta', 'W', 'Re', 'Os', 'Ir', 'Pt', 'Au', 'Hg', 'Tl', \n", "                'Pb', 'Bi', 'Po', 'At', 'Rn', 'Fr', 'Ra', 'Rf', 'Db', 'Sg', \n", "                'Bh', 'Hs', 'Mt', 'Ds', 'Rg', 'Cn', 'Fl', 'Lv', 'La', 'Ce', 'Pr', \n", "                'Nd', 'Pm', 'Sm', 'Eu', 'Gd', 'Tb', 'Dy', 'Ho', 'Er', 'Tm', 'Yb', \n", "                'Lu', 'Ac', 'Th', 'Pa', 'U', 'Np', 'Pu', 'Am', 'Cm', 'Bk', \n", "                'Cf', 'Es', 'Fm', 'Md', 'No', 'Lr']\n", "ATOM_SYMBOLS_FEATURE = CategoricalVariable('atom_symbol', ATOM_SYMBOLS)\n", "\n", "ATOM_AROMATIC_VALUES = [True, False]\n", "ATOM_AROMATIC_FEATURE = CategoricalVariable('is_aromatic', ATOM_AROMATIC_VALUES)\n", "\n", "# In practice you might like to use categroical features for valence, but we use continuous here for demonstration\n", "ATOM_EXPLICIT_VALENCE_FEATURE = ContinuousVariable('explicit_valence')\n", "\n", "ATOM_IMPLICIT_VALENCE_FEATURE = ContinuousVariable('implicit_valence')\n", "\n", "ATOM_FEATURES = [ATOM_SYMBOLS_FEATURE, ATOM_AROMATIC_FEATURE, ATOM_EXPLICIT_VALENCE_FEATURE, ATOM_IMPLICIT_VALENCE_FEATURE]\n", "\n", "def get_atom_features(rd_atom):\n", "  atom_symbol = rd_atom.GetSymbol()\n", "  is_aromatic = rd_atom.GetIsAromatic()\n", "  implicit_valence = float(rd_atom.GetImplicitValence())\n", "  explicit_valence = float(rd_atom.GetExplicitValence())\n", "  return {ATOM_SYMBOLS_FEATURE: atom_symbol,\n", "          ATOM_AROMATIC_FEATURE: is_aromatic,\n", "          ATOM_EXPLICIT_VALENCE_FEATURE: explicit_valence,\n", "          ATOM_IMPLICIT_VALENCE_FEATURE: implicit_valence}"], "execution_count": null, "outputs": []}, {"cell_type": "code", "metadata": {"id": "aXWWFqbhMIqd"}, "source": ["# We could use the RDKit enumeration types instead of strings, but the advantage\n", "# of doing it like this is that our representation becomes independent of RDKit\n", "BOND_TYPES = ['UNSPECIFIED', 'SINGLE', 'DOUBLE', 'TRIPLE', 'QUADRUPLE', \n", "              'QUINTUPLE', 'HEXTUPLE', 'ONEANDAHALF', 'TWOANDAHALF',\n", "              'THREEANDAHALF','FOURANDAHALF', 'FIVEANDAHALF', 'AROMATIC', \n", "              'IONIC', 'HYDROGEN', 'THREECENTER',\t'DATIVEONE', 'DATIVE',\n", "              'DATIVEL', 'DATIVER', 'OTHER', 'ZERO']\n", "TYPE_FEATURE = CategoricalVariable('bond_type', BOND_TYPES)\n", "\n", "BOND_DIRECTIONS = ['NONE', 'BEGINWEDGE', 'BEGINDASH', 'ENDDOWNRIGHT', 'ENDUPRIGHT', 'EITHERDOUBLE' ]\n", "DIRECTION_FEATURE = CategoricalVariable('bond_direction', BOND_DIRECTIONS)\n", "\n", "BOND_STEREO = ['STEREONONE', 'STEREOANY', 'STEREOZ', 'STEREOE', \n", "               'STEREOCIS', 'STEREOTRANS']\n", "STEREO_FEATURE = CategoricalVariable('bond_stereo', BOND_STEREO)\n", "\n", "AROMATIC_VALUES = [True, False]\n", "AROMATIC_FEATURE = CategoricalVariable('is_aromatic', AROMATIC_VALUES)\n", "\n", "BOND_FEATURES = [TYPE_FEATURE, DIRECTION_FEATURE, AROMATIC_FEATURE, STEREO_FEATURE]\n", "\n", "def get_bond_features(rd_bond):\n", "  bond_type = str(rd_bond.GetBondType())\n", "  bond_stereo_info = str(rd_bond.GetStereo())\n", "  bond_direction = str(rd_bond.GetBondDir())\n", "  is_aromatic = rd_bond.GetIsAromatic()\n", "  return {TYPE_FEATURE: bond_type,\n", "          DIRECTION_FEATURE: bond_direction,\n", "          AROMATIC_FEATURE: is_aromatic,\n", "          STEREO_FEATURE: bond_stereo_info}"], "execution_count": null, "outputs": []}, {"cell_type": "code", "metadata": {"id": "04crgcXvhI9X"}, "source": ["def rdmol_to_graph(mol):\n", "  atoms = {rd_atom.GetIdx(): get_atom_features(rd_atom) for rd_atom in mol.GetAtoms()}\n", "  bonds = {frozenset((rd_bond.GetBeginAtomIdx(), rd_bond.GetEndAtomIdx())): get_bond_features(rd_bond) for rd_bond in mol.GetBonds()}\n", "  return atoms, bonds"], "execution_count": null, "outputs": []}, {"cell_type": "code", "metadata": {"id": "k9CzqW5uyopl"}, "source": ["def smiles_to_graph(smiles):\n", "  rd_mol = MolFromSmiles(smiles)\n", "  graph = rdmol_to_graph(rd_mol)\n", "  return graph"], "execution_count": null, "outputs": []}, {"cell_type": "code", "metadata": {"id": "hxyM_A9fyxSc"}, "source": ["g = smiles_to_graph('c1ccccc1')"], "execution_count": null, "outputs": []}, {"cell_type": "code", "metadata": {"id": "-JJ3Q2-bje4m"}, "source": ["class GraphDataset(Dataset):\n", "  def __init__(self, *, graphs, labels, node_variables, edge_variables, metadata=None):\n", "    '''\n", "    Create a new graph dataset, \n", "    '''\n", "    self.graphs = graphs\n", "    self.labels = labels\n", "    assert len(self.graphs) == len(self.labels), \"The graphs and labels lists must be the same length\"\n", "    self.metadata = metadata\n", "    if self.metadata is not None:\n", "      assert len(self.metadata) == len(self.graphs), \"The metadata list needs to be as long as the graphs\"\n", "    self.node_variables = node_variables\n", "    self.edge_variables = edge_variables\n", "    self.categorical_node_variables = [var for var in self.node_variables if isinstance(var, CategoricalVariable)]\n", "    self.continuous_node_variables = [var for var in self.node_variables if isinstance(var, ContinuousVariable)]\n", "    self.categorical_edge_variables = [var for var in self.edge_variables if isinstance(var, CategoricalVariable)]\n", "    self.continuous_edge_variables = [var for var in self.edge_variables if isinstance(var, ContinuousVariable)]\n", "\n", "  def __len__(self):\n", "    return len(self.graphs)\n", "\n", "  def make_continuous_node_features(self, nodes):\n", "    if len(self.continuous_node_variables) == 0:\n", "      return None\n", "    n_nodes = len(nodes)\n", "    n_features = len(self.continuous_node_variables)\n", "    continuous_node_features = torch.zeros((n_nodes, n_features), dtype=float_type)\n", "    for node_idx, features in nodes.items():\n", "      node_features = torch.tensor([features[continuous_feature] for continuous_feature in self.continuous_node_variables], dtype=float_type)\n", "      continuous_node_features[node_idx] = node_features\n", "    return continuous_node_features\n", "      \n", "  def make_categorical_node_features(self, nodes):\n", "    if len(self.categorical_node_variables) == 0:\n", "      return None\n", "    n_nodes = len(nodes)\n", "    n_features = len(self.categorical_node_variables)\n", "    categorical_node_features = torch.zeros((n_nodes, n_features), dtype=categorical_type)\n", "\n", "    for node_idx, features in nodes.items():\n", "      for i, categorical_variable in enumerate(self.categorical_node_variables):\n", "          value = features[categorical_variable]\n", "          value_index = categorical_variable.value_to_idx(value)\n", "          categorical_node_features[node_idx, i] = value_index\n", "\n", "    return categorical_node_features\n", "\n", "  def make_continuous_edge_features(self, n_nodes, edges):\n", "    if len(self.continuous_edge_variables) == 0:\n", "      return None\n", "    n_features = len(self.continuous_edge_variables)\n", "    continuous_edge_features = torch.zeros((n_nodes, n_nodes, n_features), dtype=float_type)\n", "    for edge, features in edges.items():\n", "      edge_features = torch.tensor([features[continuous_feature] for continuous_feature in self.continuous_edge_variables], dtype=float_type)\n", "      u,v = edge\n", "      continuous_edge_features[u, v] = edge_features\n", "      if isinstance(edge, Set):\n", "        continuous_edge_features[v, u] = edge_features\n", "\n", "    return continuous_edge_features\n", "\n", "  def make_categorical_edge_features(self, n_nodes, edges):\n", "    if len(self.categorical_edge_variables) == 0:\n", "      return None\n", "    n_features = len(self.categorical_edge_variables)\n", "    categorical_edge_features = torch.zeros((n_nodes, n_nodes, n_features), dtype=categorical_type)\n", "\n", "    for edge, features in edges.items():\n", "      u,v = edge\n", "      for i, categorical_variable in enumerate(self.categorical_edge_variables):\n", "          value = features[categorical_variable]\n", "          value_index = categorical_variable.value_to_idx(value)\n", "          categorical_edge_features[u, v, i] = value_index\n", "          if isinstance(edge, Set):\n", "            categorical_edge_features[v, u, i] = value_index\n", "\n", "    return categorical_edge_features\n", "  \n", "  def __getitem__(self, index):\n", "    # This is where the important stuff happens. We use our node and \n", "    # edge variable attributes to select what node and edge features to use.\n", "    # In practice, we often do this as a pre-processing step, but here we do it \n", "    # in the getitem function for clarity\n", "\n", "    graph = self.graphs[index]\n", "    nodes, edges = graph\n", "    n_nodes = len(nodes)\n", "    continuous_node_features = self.make_continuous_node_features(nodes)\n", "    categorical_node_features = self.make_categorical_node_features(nodes)\n", "    continuous_edge_features = self.make_continuous_edge_features(n_nodes, edges)\n", "    categorical_edge_features = self.make_categorical_edge_features(n_nodes, edges)\n", "\n", "    label = self.labels[index]\n", "\n", "    nodes_idx = sorted(nodes.keys())\n", "    edge_list = sorted(edges.keys())\n", "\n", "    n_nodes = len(nodes)\n", "    adjacency_matrix = torch.zeros((n_nodes, n_nodes), dtype=float_type)\n", "    for edge in edges:\n", "      u, v = edge\n", "      adjacency_matrix[u,v] = 1\n", "      if isinstance(edge, Set):\n", "        # This edge is unordered, assume this is a undirected graph\n", "        adjacency_matrix[v,u] = 1\n", "\n", "    adjacency_list = defaultdict(list)\n", "    for edge in edges:\n", "      u,v = edge\n", "      adjacency_list[u].append(v)\n", "      # Assume undirected graph is the edge is a set\n", "      if isinstance(edge, Set):\n", "        adjacency_list[v].append(u)\n", "\n", "    data_record = {'nodes': nodes_idx,\n", "                   'adjacency_matrix': adjacency_matrix,\n", "                   'adjacency_list': adjacency_list,\n", "                   'categorical_node_features': categorical_node_features,\n", "                   'continuous_node_features': continuous_node_features,\n", "                   'categorical_edge_features': categorical_edge_features,\n", "                   'continuous_edge_features': continuous_edge_features,\n", "                   'label': label}\n", "\n", "    # If you need to add extra information (metadata about this graph) you can \n", "    # add an extra key-value pair here. The advantage of using a dict compared \n", "    # to a tuple is that the downstreams code doesn't break as long as at least \n", "    # the expected keys are present. The downside is that using a dict adds \n", "    # overhead (accessing a dict compared to unpacking a tuple).\n", "    # A more robust implementation might actually make a separate class for \n", "    # dataset entires\n", "    if self.metadata is not None:\n", "      data_record['metadata'] = self.metadata[index]\n", "    return data_record\n", "\n", "  def get_node_variables(self):\n", "    return {'continuous': self.continuous_node_variables,\n", "            'categorical': self.categorical_node_variables}\n", "  \n", "  def get_edge_variables(self):\n", "    return {'continuous': self.continuous_edge_variables,\n", "            'categorical': self.categorical_edge_variables}"], "execution_count": null, "outputs": []}, {"cell_type": "code", "metadata": {"id": "QFJdou8MmjEu"}, "source": ["def make_molecular_graph_dataset(smiles_records, atom_features=ATOM_FEATURES, bond_features=BOND_FEATURES):\n", "  '''\n", "  Create a new GraphDataset from a list of smiles_records dictionaries.\n", "  These records should contain the key 'smiles' and 'label'. Any other keys will be saved as a 'metadata' record.\n", "  '''\n", "  graphs = []\n", "  labels = []\n", "  metadata = []\n", "  for smiles_record in smiles_records:\n", "    smiles = smiles_record['smiles']\n", "    label = smiles_record['label']\n", "    graph = smiles_to_graph(smiles)\n", "    graphs.append(graph)\n", "    labels.append(label)\n", "    metadata.append(smiles_record)\n", "  return GraphDataset(graphs=graphs, \n", "                      labels=labels, \n", "                      node_variables=atom_features, \n", "                      edge_variables=bond_features, \n", "                      metadata=metadata)\n", "  "], "execution_count": null, "outputs": []}, {"cell_type": "code", "metadata": {"id": "U-oswiwr0H4j"}, "source": ["dataset = make_molecular_graph_dataset([{'smiles': 'c1ccccc1', 'label':1},{'smiles':'OS(=O)(=O)O', 'label': 0}])"], "execution_count": null, "outputs": []}, {"cell_type": "code", "metadata": {"id": "-N10j3k2YiLA"}, "source": ["from collections.abc import Set # We assume that edges as sets are for undirected graphs\n", "\n", "def collate_graph_batch(batch):\n", "  '''Collate a batch of graph dictionaries produdce by a GraphDataset'''\n", "  batch_size = len(batch)\n", "\n", "  max_nodes = max(len(graph['nodes']) for graph in batch)\n", "  \n", "  # We start by allocating the tensors we'll use. We defer allocating feature\n", "  # tensors until we know the graphs actually has those kinds of features.\n", "  adjacency_matrices = torch.zeros((batch_size, max_nodes, max_nodes), dtype=float_type)\n", "  labels = torch.tensor([graph['label'] for graph in batch], dtype=labels_type)\n", "  stacked_continuous_node_features = None\n", "  stacked_categorical_node_features = None\n", "  stacked_continuous_edge_features = None\n", "  stacked_categorical_edge_features = None\n", "\n", "  nodes_mask = torch.zeros((batch_size, max_nodes), dtype=mask_type)\n", "  edge_mask = torch.zeros((batch_size, max_nodes, max_nodes), dtype=mask_type)\n", "  \n", "  has_metadata = False\n", "\n", "  for i, graph in enumerate(batch):\n", "    if 'metadata' in graph:\n", "      has_metadata = True\n", "    # We'll take basic information about the different graphs from the adjacency \n", "    # matrix\n", "    adjacency_matrix = graph['adjacency_matrix']\n", "    g_nodes, g_nodes = adjacency_matrix.shape\n", "    adjacency_matrices[i, :g_nodes, :g_nodes] = adjacency_matrix\n", "\n", "    # Now when we know how many of the entries are valid, we set those to 1s in\n", "    # the masks\n", "    edge_mask[i, :g_nodes, :g_nodes] = 1\n", "    nodes_mask[i, :g_nodes] = 1\n", "    \n", "\n", "    # All the feature constructions follow the same recipie. We essentially\n", "    # locate the entries in the stacked feature tensor (containing all graphs)\n", "    # and set it with the features from the current graph.\n", "    g_continuous_node_features = graph['continuous_node_features']\n", "    if g_continuous_node_features is not None:\n", "      if stacked_continuous_node_features is None:\n", "        g_nodes, num_features = g_continuous_node_features.shape\n", "        stacked_continuous_node_features = torch.zeros((batch_size, max_nodes, num_features))\n", "      stacked_continuous_node_features[i, :g_nodes] = g_continuous_node_features\n", "    \n", "    g_categorical_node_features = graph['categorical_node_features']\n", "    if g_categorical_node_features is not None:\n", "      if stacked_categorical_node_features is None:\n", "        g_nodes, num_features = g_categorical_node_features.shape\n", "        stacked_categorical_node_features = torch.zeros((batch_size, max_nodes, num_features), dtype=categorical_type)\n", "      stacked_categorical_node_features[i, :g_nodes] = g_categorical_node_features\n", "\n", "    g_continuous_edge_features = graph['continuous_edge_features']\n", "    if g_continuous_edge_features is not None:\n", "      if stacked_continuous_edge_features is None:\n", "        g_nodes, g_nodes, num_features = g_continuous_edge_features.shape\n", "        stacked_continuous_edge_features = torch.zeros((batch_size, max_nodes, max_nodes, num_features))\n", "      stacked_continuous_edge_features[i, :g_nodes, :g_nodes] = g_continuous_edge_features\n", "\n", "    g_categorical_edge_features = graph['categorical_edge_features']\n", "    if g_categorical_edge_features is not None:\n", "      if stacked_categorical_edge_features is None:\n", "        g_nodes, g_nodes, num_features = g_categorical_edge_features.shape\n", "        stacked_categorical_edge_features = torch.zeros((batch_size, max_nodes, max_nodes, num_features), dtype=categorical_type)\n", "      stacked_categorical_edge_features[i, :g_nodes, :g_nodes] = g_categorical_edge_features\n", "\n", "\n", "  batch_record = {'adjacency_matrices': adjacency_matrices,\n", "          'categorical_node_features': stacked_categorical_node_features,\n", "          'continuous_node_features': stacked_continuous_node_features,\n", "          'categorical_edge_features': stacked_categorical_edge_features,\n", "          'continuous_edge_features': stacked_continuous_edge_features,\n", "          'nodes_mask': nodes_mask,\n", "          'edge_mask': edge_mask,\n", "          'labels': labels}\n", "  if has_metadata:\n", "    batch_record['metadata'] = [g['metadata'] for g in batch]\n", "\n", "  return batch_record\n"], "execution_count": null, "outputs": []}, {"cell_type": "code", "metadata": {"id": "kjla93qn0YOx"}, "source": ["example_batch = collate_graph_batch([dataset[0], dataset[1]])"], "execution_count": null, "outputs": []}, {"cell_type": "code", "metadata": {"id": "cy2k-LB2LvAp"}, "source": ["from torch.nn import Module\n", "class Embedder(Module):\n", "  def __init__(self, categorical_variables, embedding_dim):\n", "    super().__init__()\n", "    self.categorical_variables = categorical_variables\n", "    embeddings = []\n", "    for var in categorical_variables:\n", "      num_embeddings = len(var)\n", "      if var.has_null_value:\n", "        # It's not uncommon to have missing values, we support this assinging a special 0-index which have the zero-vector as its embedding\n", "        embedding = Embedding(num_embeddings, embedding_dim, padding_idx=var.get_null_idx())\n", "      else:\n", "        embedding = Embedding(num_embeddings, embedding_dim)\n", "      embeddings.append(embedding)\n", "    self.embeddings = ModuleList(embeddings)\n", "    \n", "  \n", "  def forward(self, categorical_features):\n", "    # The node features is a matrix with as many rows as nodes of our graph\n", "    # and as many columns as we have categorical features\n", "    all_embedded_vars = []\n", "    for i, embedding in enumerate(self.embeddings):\n", "      # We pick out just the i'th column. The ellipsis '...' in a numpy-style \n", "      # slice is a useful way of saying you want full range over all other axises\n", "      # We use it so that this can actually take a categorical_features array\n", "      # with arbitrary number of trailing axises to support both the node \n", "      # features, the edge features and the mini-batched version of both\n", "      var_indices = categorical_features[..., i]  \n", "      embedded_vars = embedding(var_indices)\n", "      all_embedded_vars.append(embedded_vars)\n", "\n", "    # If you like, you can implement concatenation instead of sum here\n", "    stacked_embedded_vars = torch.stack(all_embedded_vars, dim=0)\n", "    embedded_vars = torch.sum(stacked_embedded_vars, dim=0)\n", "    return embedded_vars"], "execution_count": null, "outputs": []}, {"cell_type": "code", "metadata": {"id": "pqoEnLmETVcJ"}, "source": ["class FeatureCombiner(Module):\n", "  def __init__(self, categorical_variables, embedding_dim):\n", "    super().__init__()\n", "    self.categorical_variables = categorical_variables\n", "    self.embedder = Embedder(self.categorical_variables, embedding_dim)\n", "    \n", "  def forward(self, continuous_features, categorical_features, ):\n", "    # We need to be agnostic to whether we have categorical features and continuous features (it's not uncommon to only use one kind)\n", "    features = []\n", "    if categorical_features is not None:\n", "      embedded_features = self.embedder(categorical_features)\n", "      features.append(embedded_features)\n", "      # The embedded features are now of shape (n_nodes, embedding_dim)\n", "    if continuous_features is not None:\n", "      features.append(continuous_features)\n", "    if len(features) == 0:\n", "      raise RuntimeError('No features to combine')\n", "    full_features = torch.cat(features, dim=-1)  # Now we concatenate along the feature dimension\n", "    return full_features\n"], "execution_count": null, "outputs": []}, {"cell_type": "markdown", "metadata": {"id": "3Ve3FVz24SZh"}, "source": ["## Training step\n", "This section downloads the data and implements the training procedures. This code should also be familiar from the previous lessons."]}, {"cell_type": "code", "metadata": {"id": "0fRkH33ZnS5f"}, "source": ["# Well start by downloading the dataset. We're relying on the direct download link from MoleculeNet\n", "! wget -q https://deepchemdata.s3-us-west-1.amazonaws.com/datasets/BBBP.csv"], "execution_count": null, "outputs": []}, {"cell_type": "code", "metadata": {"id": "Tmk0hB3snikU"}, "source": ["import pandas as pd\n", "from collections import defaultdict, deque # We'll use this to construct the dataset splits\n", "from rdkit.Chem.Scaffolds.MurckoScaffold import MurckoScaffoldSmilesFromSmiles"], "execution_count": null, "outputs": []}, {"cell_type": "code", "metadata": {"colab": {"base_uri": "https://localhost:8080/", "height": 424}, "id": "9vKIE5_yqwHA", "executionInfo": {"status": "ok", "timestamp": 1637726958262, "user_tz": -60, "elapsed": 6, "user": {"displayName": "Erik Ylip\u00e4\u00e4", "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Ghv2ZnBVJIOqdwt_HBhCMHTtEbc2FUGzMgmKQEW7VE=s64", "userId": "10747062110949017925"}}, "outputId": "791f5bdd-9437-41bc-f4f7-340fde7c7414"}, "source": ["bbbp_table = pd.read_csv('BBBP.csv')\n", "bbbp_table"], "execution_count": null, "outputs": [{"output_type": "execute_result", "data": {"text/html": ["<div>\n", "<style scoped>\n", "    .dataframe tbody tr th:only-of-type {\n", "        vertical-align: middle;\n", "    }\n", "\n", "    .dataframe tbody tr th {\n", "        vertical-align: top;\n", "    }\n", "\n", "    .dataframe thead th {\n", "        text-align: right;\n", "    }\n", "</style>\n", "<table border=\"1\" class=\"dataframe\">\n", "  <thead>\n", "    <tr style=\"text-align: right;\">\n", "      <th></th>\n", "      <th>num</th>\n", "      <th>name</th>\n", "      <th>p_np</th>\n", "      <th>smiles</th>\n", "    </tr>\n", "  </thead>\n", "  <tbody>\n", "    <tr>\n", "      <th>0</th>\n", "      <td>1</td>\n", "      <td>Propanolol</td>\n", "      <td>1</td>\n", "      <td>[Cl].CC(C)NCC(O)COc1cccc2ccccc12</td>\n", "    </tr>\n", "    <tr>\n", "      <th>1</th>\n", "      <td>2</td>\n", "      <td>Terbutylchlorambucil</td>\n", "      <td>1</td>\n", "      <td>C(=O)(OC(C)(C)C)CCCc1ccc(cc1)N(CCCl)CCCl</td>\n", "    </tr>\n", "    <tr>\n", "      <th>2</th>\n", "      <td>3</td>\n", "      <td>40730</td>\n", "      <td>1</td>\n", "      <td>c12c3c(N4CCN(C)CC4)c(F)cc1c(c(C(O)=O)cn2C(C)CO...</td>\n", "    </tr>\n", "    <tr>\n", "      <th>3</th>\n", "      <td>4</td>\n", "      <td>24</td>\n", "      <td>1</td>\n", "      <td>C1CCN(CC1)Cc1cccc(c1)OCCCNC(=O)C</td>\n", "    </tr>\n", "    <tr>\n", "      <th>4</th>\n", "      <td>5</td>\n", "      <td>cloxacillin</td>\n", "      <td>1</td>\n", "      <td>Cc1onc(c2ccccc2Cl)c1C(=O)N[C@H]3[C@H]4SC(C)(C)...</td>\n", "    </tr>\n", "    <tr>\n", "      <th>...</th>\n", "      <td>...</td>\n", "      <td>...</td>\n", "      <td>...</td>\n", "      <td>...</td>\n", "    </tr>\n", "    <tr>\n", "      <th>2045</th>\n", "      <td>2049</td>\n", "      <td>licostinel</td>\n", "      <td>1</td>\n", "      <td>C1=C(Cl)C(=C(C2=C1NC(=O)C(N2)=O)[N+](=O)[O-])Cl</td>\n", "    </tr>\n", "    <tr>\n", "      <th>2046</th>\n", "      <td>2050</td>\n", "      <td>ademetionine(adenosyl-methionine)</td>\n", "      <td>1</td>\n", "      <td>[C@H]3([N]2C1=C(C(=NC=N1)N)N=C2)[C@@H]([C@@H](...</td>\n", "    </tr>\n", "    <tr>\n", "      <th>2047</th>\n", "      <td>2051</td>\n", "      <td>mesocarb</td>\n", "      <td>1</td>\n", "      <td>[O+]1=N[N](C=C1[N-]C(NC2=CC=CC=C2)=O)C(CC3=CC=...</td>\n", "    </tr>\n", "    <tr>\n", "      <th>2048</th>\n", "      <td>2052</td>\n", "      <td>tofisoline</td>\n", "      <td>1</td>\n", "      <td>C1=C(OC)C(=CC2=C1C(=[N+](C(=C2CC)C)[NH-])C3=CC...</td>\n", "    </tr>\n", "    <tr>\n", "      <th>2049</th>\n", "      <td>2053</td>\n", "      <td>azidamfenicol</td>\n", "      <td>1</td>\n", "      <td>[N+](=NCC(=O)N[C@@H]([C@H](O)C1=CC=C([N+]([O-]...</td>\n", "    </tr>\n", "  </tbody>\n", "</table>\n", "<p>2050 rows \u00d7 4 columns</p>\n", "</div>"], "text/plain": ["       num  ...                                             smiles\n", "0        1  ...                   [Cl].CC(C)NCC(O)COc1cccc2ccccc12\n", "1        2  ...           C(=O)(OC(C)(C)C)CCCc1ccc(cc1)N(CCCl)CCCl\n", "2        3  ...  c12c3c(N4CCN(C)CC4)c(F)cc1c(c(C(O)=O)cn2C(C)CO...\n", "3        4  ...                   C1CCN(CC1)Cc1cccc(c1)OCCCNC(=O)C\n", "4        5  ...  Cc1onc(c2ccccc2Cl)c1C(=O)N[C@H]3[C@H]4SC(C)(C)...\n", "...    ...  ...                                                ...\n", "2045  2049  ...    C1=C(Cl)C(=C(C2=C1NC(=O)C(N2)=O)[N+](=O)[O-])Cl\n", "2046  2050  ...  [C@H]3([N]2C1=C(C(=NC=N1)N)N=C2)[C@@H]([C@@H](...\n", "2047  2051  ...  [O+]1=N[N](C=C1[N-]C(NC2=CC=CC=C2)=O)C(CC3=CC=...\n", "2048  2052  ...  C1=C(OC)C(=CC2=C1C(=[N+](C(=C2CC)C)[NH-])C3=CC...\n", "2049  2053  ...  [N+](=NCC(=O)N[C@@H]([C@H](O)C1=CC=C([N+]([O-]...\n", "\n", "[2050 rows x 4 columns]"]}, "metadata": {}, "execution_count": 21}]}, {"cell_type": "code", "metadata": {"colab": {"base_uri": "https://localhost:8080/"}, "id": "oncEnL6Ywfhn", "executionInfo": {"status": "ok", "timestamp": 1637726961737, "user_tz": -60, "elapsed": 694, "user": {"displayName": "Erik Ylip\u00e4\u00e4", "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Ghv2ZnBVJIOqdwt_HBhCMHTtEbc2FUGzMgmKQEW7VE=s64", "userId": "10747062110949017925"}}, "outputId": "a79f3b9f-7216-4506-c46d-48b9eea3ebf4"}, "source": ["# There are about 50 problematic SMILES. We've supressed RDKit logger outputs\n", "# but if you haven't you'll get a lot of printouts here\n", "# 11 SMILES can't be processed at all so we throw them away\n", "\n", "smiles_records = []\n", "for i, num, name, p_np, smiles in bbbp_table.to_records():\n", "  # check if RDKit accepts this smiles\n", "  if MolFromSmiles(smiles) is not None:\n", "    smiles_record = {'smiles': smiles, 'label': p_np, 'metadata': {'row': i}}\n", "    smiles_records.append(smiles_record)\n", "  else:\n", "    print(f'Molecule {smiles} on row {i} could not be parsed by RDKit')\n", "  "], "execution_count": null, "outputs": [{"output_type": "stream", "name": "stdout", "text": ["Molecule O=N([O-])C1=C(CN=C1NCCSCc2ncccc2)Cc3ccccc3 on row 59 could not be parsed by RDKit\n", "Molecule c1(nc(NC(N)=[NH2])sc1)CSCCNC(=[NH]C#N)NC on row 61 could not be parsed by RDKit\n", "Molecule Cc1nc(sc1)\\[NH]=C(\\N)N on row 391 could not be parsed by RDKit\n", "Molecule s1cc(CSCCN\\C(NC)=[NH]\\C#N)nc1\\[NH]=C(\\N)N on row 614 could not be parsed by RDKit\n", "Molecule c1c(c(ncc1)CSCCN\\C(=[NH]\\C#N)NCC)Br on row 642 could not be parsed by RDKit\n", "Molecule n1c(csc1\\[NH]=C(\\N)N)c1ccccc1 on row 645 could not be parsed by RDKit\n", "Molecule n1c(csc1\\[NH]=C(\\N)N)c1cccc(c1)N on row 646 could not be parsed by RDKit\n", "Molecule n1c(csc1\\[NH]=C(\\N)N)c1cccc(c1)NC(C)=O on row 647 could not be parsed by RDKit\n", "Molecule n1c(csc1\\[NH]=C(\\N)N)c1cccc(c1)N\\C(NC)=[NH]\\C#N on row 648 could not be parsed by RDKit\n", "Molecule s1cc(nc1\\[NH]=C(\\N)N)C on row 649 could not be parsed by RDKit\n", "Molecule c1(cc(N\\C(=[NH]\\c2cccc(c2)CC)C)ccc1)CC on row 685 could not be parsed by RDKit\n"]}]}, {"cell_type": "code", "metadata": {"id": "-gjVLCOJyhQr"}, "source": ["import random\n", "random.seed(1729)\n", "\n", "training_fraction = 0.8\n", "dev_fraction = 0.1\n", "n_examples = len(smiles_records)\n", "n_training_examples = int(n_examples*training_fraction)\n", "n_dev_examples = int(n_examples*dev_fraction)\n", "\n", "indices = list(range(n_examples))\n", "random.shuffle(indices)  # shuffle is in place\n", "training_indices = indices[:n_training_examples]\n", "dev_indices = indices[n_training_examples:n_training_examples+n_dev_examples]\n", "test_indices = indices[n_training_examples+n_dev_examples:]\n", "\n", "training_smiles_records = [smiles_records[i] for i in training_indices]\n", "dev_smiles_records = [smiles_records[i] for i in dev_indices]\n", "test_smiles_records = [smiles_records[i] for i in test_indices]"], "execution_count": null, "outputs": []}, {"cell_type": "code", "metadata": {"id": "OM-kOaPtKrsI"}, "source": ["training_graph_dataset = make_molecular_graph_dataset(training_smiles_records)\n", "dev_graph_dataset = make_molecular_graph_dataset(dev_smiles_records)\n", "test_graph_dataset = make_molecular_graph_dataset(test_smiles_records)"], "execution_count": null, "outputs": []}, {"cell_type": "code", "metadata": {"id": "X99tB3D5T-j0"}, "source": ["from torch.utils.data import DataLoader\n", "\n", "batch_size=32\n", "num_dataloader_workers=2 # The colab instances are very limited in number of cpus\n", "\n", "training_dataloader = DataLoader(training_graph_dataset, \n", "                                          batch_size=batch_size, \n", "                                          shuffle=True, \n", "                                          num_workers=num_dataloader_workers, \n", "                                          collate_fn=collate_graph_batch)\n", "dev_dataloader = DataLoader(dev_graph_dataset, \n", "                                     batch_size=batch_size, \n", "                                     shuffle=False, \n", "                                     num_workers=num_dataloader_workers, \n", "                                     drop_last=False, \n", "                                     collate_fn=collate_graph_batch)\n", "test_dataloader = DataLoader(test_graph_dataset, \n", "                                     batch_size=batch_size, \n", "                                     shuffle=False, \n", "                                     num_workers=num_dataloader_workers, \n", "                                     drop_last=False, \n", "                                     collate_fn=collate_graph_batch)"], "execution_count": null, "outputs": []}, {"cell_type": "code", "metadata": {"id": "7a5wBTtqhbwt"}, "source": ["class GraphPredictionHeadConfig:\n", "  def __init__(self, *, d_model, ffn_dim, pooling_type='sum'):\n", "    # Pooling type can be 'sum' or 'mean'\n", "    self.d_model = d_model\n", "    self.ffn_dim = ffn_dim\n", "    self.pooling_type = pooling_type\n", "\n", "class GraphPredictionHead(Module):\n", "  def __init__(self, input_dim, output_dim, config):\n", "    super().__init__()\n", "    self.input_dim = input_dim\n", "    self.output_dim = output_dim\n", "    self.config = config\n", "    self.predictor = Sequential(Linear(self.input_dim, self.config.ffn_dim), \n", "                                ReLU(), \n", "                                Linear(self.config.ffn_dim, self.output_dim))\n", "\n", "  def forward(self, node_features, node_mask):\n", "    # The node_features is a tensor of shape (*leading_axises, max_nodes, d_model)\n", "    # We want to 'pool' this along the node-axis, which is dim=-2 in pytorch terms\n", "    # In this case we assume these features are valid, i.e. that they have been\n", "    # masked at a previous step.\n", "    if self.config.pooling_type == 'sum':\n", "      pooled_nodes = node_features.sum(dim=-2)\n", "    elif self.config.pooling_type == 'mean':\n", "      # We can't take just the mean along dim=-2, since if this is a batch, some of \n", "      # the graphs need to be divided by a smaller number than max_nodes. \n", "      # Thankfully we have the information about how many nodes each graph has in\n", "      # the node_mask, and since it has 1's and 0's, just summing it along the \n", "      # max_nodes axis gives the count of nodes for the corresponding graph\n", "      \n", "      # node_mask has the shape (batch_size, max_nodes), or just (max_nodes,) \n", "      # if it's not a batch. We get the count per graph by reducing along the \n", "      # last dimension, and by setting keepdims=True, we get a shape \n", "      # (batch_size, 1) or (1,) which will allow for broadcasting this with \n", "      # division over the summed feature vectors to calculate their mean \n", "      node_counts = node_mask.sum(dim=-1)  \n", "      summed_feature_vectors = node_features.sum(dim=-2)\n", "      pooled_nodes = summed_feature_vectors/node_counts\n", "    else:\n", "      raise ValueError(f'Unsupported pooling type {self.config.pooling_type}')\n", "    \n", "    prediction = self.predictor(pooled_nodes)\n", "    return prediction"], "execution_count": null, "outputs": []}, {"cell_type": "code", "metadata": {"id": "TFi9AezswiNr"}, "source": ["from tqdm.notebook import tqdm, trange\n", "from torch.nn import BCEWithLogitsLoss\n", "from torch.optim import AdamW\n", "from sklearn.metrics import roc_auc_score"], "execution_count": null, "outputs": []}, {"cell_type": "code", "metadata": {"colab": {"base_uri": "https://localhost:8080/"}, "id": "8ePjtAzpwiNr", "executionInfo": {"status": "ok", "timestamp": 1637726986204, "user_tz": -60, "elapsed": 527, "user": {"displayName": "Erik Ylip\u00e4\u00e4", "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Ghv2ZnBVJIOqdwt_HBhCMHTtEbc2FUGzMgmKQEW7VE=s64", "userId": "10747062110949017925"}}, "outputId": "b7015a3b-87ef-4aad-c477-8b027fa9a0d3"}, "source": ["if torch.cuda.is_available():\n", "  device = torch.device('cuda')\n", "else:\n", "  device = torch.device('cpu')\n", "print(\"Device is\", device)"], "execution_count": null, "outputs": [{"output_type": "stream", "name": "stdout", "text": ["Device is cuda\n"]}]}, {"cell_type": "code", "metadata": {"id": "cxlNWD6WUwyh"}, "source": ["from tqdm.notebook import tqdm, trange\n", "from torch.nn import BCEWithLogitsLoss, MSELoss\n", "from torch.optim import AdamW\n", "from sklearn.metrics import roc_auc_score, mean_squared_error, mean_absolute_error, median_absolute_error\n", "import matplotlib.pyplot as plt\n", "import seaborn as sns\n", "\n", "def batch_to_device(batch, device):\n", "  moved_batch = {}\n", "  for k, v in batch.items():\n", "    if torch.is_tensor(v):\n", "      v = v.to(device)\n", "    moved_batch[k] = v\n", "  return moved_batch\n", "\n", "class Trainer:\n", "  def __init__(self, *, model, \n", "               loss_fn, training_dataloader, \n", "               dev_dataloader, device=device):\n", "    self.model = model\n", "    self.training_dataloader = training_dataloader\n", "    self.dev_dataloader = dev_dataloader\n", "    self.device = device\n", "    self.model.to(device)\n", "    self.total_epochs = 0\n", "    self.optimizer = AdamW(self.model.parameters(), lr=1e-4)\n", "    self.loss_fn = loss_fn\n", "\n", "  def train(self, epochs):\n", "    with trange(epochs, desc='Epoch', position=0) as epoch_progress:\n", "      batches_per_epoch = len(self.training_dataloader) + len(self.dev_dataloader)\n", "      for epoch in epoch_progress:\n", "        train_loss = 0\n", "        train_n = 0\n", "        for i, training_batch in enumerate(tqdm(self.training_dataloader, desc='Training batch', leave=False)):\n", "          self.optimizer.zero_grad()\n", "          # Move all tensors to the device\n", "          self.model.train()\n", "          training_batch = batch_to_device(training_batch, self.device)\n", "          prediction = self.model(training_batch)\n", "          labels = training_batch['labels']\n", "          loss = self.loss_fn(prediction.squeeze(), labels) # By default the predictions have shape (batch_size, 1)\n", "          loss.backward()\n", "          self.optimizer.step()\n", "          batch_n = len(labels)\n", "          train_loss += batch_n * loss.cpu().item()\n", "          train_n += batch_n\n", "        #print(f\"Training loss for epoch {total_epochs}\", train_loss/train_n)\n", "        self.total_epochs += 1\n", "\n", "        dev_predictions = []\n", "        dev_labels = []\n", "        dev_n = 0\n", "        dev_loss = 0\n", "        for i, dev_batch in enumerate(tqdm(self.dev_dataloader, desc=\"Dev batch\", leave=False)):\n", "          self.model.eval()\n", "          with torch.no_grad():\n", "            dev_batch = batch_to_device(dev_batch, self.device)\n", "            prediction = self.model(dev_batch).squeeze()\n", "            dev_predictions.extend(prediction.tolist())\n", "            labels = dev_batch['labels']\n", "            dev_labels.extend(labels.tolist())\n", "            loss = self.loss_fn(prediction, labels) # By default the predictions have shape (batch_size, 1)\n", "            batch_n = len(labels)\n", "            dev_loss += batch_n*loss.cpu().item()\n", "            dev_n += batch_n\n", "        epoch_progress.set_description(f\"Epoch: train loss {train_loss/train_n: .3f}, dev loss {dev_loss/dev_n: .3f}\")\n", "\n", "\n", "def evaluate_model(trainer, dataloader, label=None, hue_order=[0,1]):\n", "  eval_predictions = []\n", "  eval_labels = []\n", "  eval_loss = 0\n", "  eval_n = 0\n", "  model = trainer.model\n", "  loss_fn = trainer.loss_fn\n", "  total_epochs = trainer.total_epochs\n", "  for i, eval_batch in enumerate(tqdm(dataloader, desc='batch')):\n", "    model.eval()\n", "    with torch.no_grad():\n", "      eval_batch = batch_to_device(eval_batch, device)\n", "      prediction = model(eval_batch).squeeze()\n", "      eval_predictions.extend(prediction.tolist())\n", "      labels = eval_batch['labels']\n", "      eval_labels.extend(labels.tolist())\n", "      loss = loss_fn(prediction, labels) # By default the predictions have shape (batch_size, 1)\n", "      batch_n = len(labels)\n", "      eval_loss += batch_n*loss.cpu().item()\n", "      eval_n += batch_n\n", "  average_loss = eval_loss/eval_n\n", "  roc_auc = roc_auc_score(eval_labels, eval_predictions)\n", "  eval_df = pd.DataFrame(data={'target': eval_labels, 'predictions': eval_predictions})\n", "  sns.kdeplot(data=eval_df, x='predictions', hue='target', hue_order=hue_order)\n", "  sns.rugplot(data=eval_df, x='predictions', hue='target', hue_order=hue_order)\n", "  \n", "  if label is not None:\n", "    title = f\"{label} dataset after {total_epochs} epochs\\nloss {average_loss}\\nROC AUC {roc_auc}\"\n", "  else:\n", "    title = f\"After {total_epochs} epochs\\nloss {average_loss}\\nROC AUC {roc_auc}\"\n", "  plt.title(title)"], "execution_count": null, "outputs": []}, {"cell_type": "markdown", "metadata": {"id": "8xK3KYvufAfd"}, "source": ["## From GNNs to Transformers\n", "We've seen how the idea of using sums of vectors can be used to learn things from a graph, we used it on graph neighbourhoods to introduce more structure and we used it as a pooling method for graph predictions. We also saw how the aggregation of a graph neighourhood good be done by multiplying with the adjacency matrix.\n", "\n", "We saw how this summing of local neighbourhoods in the graph can be done using a matrix multiplication of the graphs adjacency matrix and the neighbouring node representations\n", "\n", "$$ \\mathbf{h}_i^L = f(\\sum_{j \\in N(i)} \\mathbf{h}_j^{L-1}, \\mathbf{h}_i^{L-1}) $$\n", "$$ \\sum_{j \\in N(i)} \\mathbf{h}_j^{L-1} = A_i H^{L-1}$$\n", "$$ H^{L-1} = \\begin{bmatrix}\\mathbf{h}_1^{L-1}\\\\\\mathbf{h}_2^{L-1}\\\\\\vdots\\\\\\mathbf{h}_n^{L-1}\\end{bmatrix} $$\n", "\n", "Assuming that the adjacency matrix $A$ is a binary indicator matrix with 1's for pairs $i,j$ which are connected by an edge, and 0 if they are not connected.\n", "\n", "A way to think about the \"convolutional\" part of a Graph Neural Network is then that it's essentially performing a matrix multiplication with the adjacency \n", "matrix:\n", "$$H^L = f(AH^{L-1}, H^{L-1})$$\n", "\n", "Where $f$ is a function we wish to learn (e.g. a neural network with two input vectors).\n", "\n", "The multiplication $AH^{L-1}$ is what takes the graph structure into account by only aggregating the local neighbourhoods. We've seen previously how this method has some fundamental limitations, in particular the \"convolution\" has a receptive field, so is biased towards learning local patterns.\n", "\n"]}, {"cell_type": "markdown", "source": ["### Dynamically computed \"adjacency matrix\"\n", "What if instead of assuming the matrix A to be the adjacenct matrix, we calcualte it's values based on the values of the node pairs and the edge? This would effectively mean that we will potentially aggregate \"neighbourhoods\" which is the complete graph. \n", "\n", "$$A = \\begin{bmatrix} g(h_1, h_1, e_{1,1})& g(h_1, h_2, e_{1,2}) & \\dots  \\\\ \\vdots& \\ddots & \\\\ g(h_n, h_1, e_{n,1})& \\dots & g(h_n, h_n, e_{n,n}) \\end{bmatrix}$$\n", "\n", "This way we can learn to use node information as well as edge information when deciding on how to aggregate the node set. While it allows us to take the graph structure into account, it also allows the model to learn relationships about more distant nodes.\n", "\n", "This is the fundamental idea of the _Transformer_ architecture. By dynamically setting the values of this \"adjacency matrix\", we can have a model which can _learn_ to induce structure from arbitrary sets. \n", "\n", "Depending on how we construct $g$, we can choose to inject knowledge about the elements of the input set $H$ such as relative position between tokens if H is actually a sequence, or information about a particular pair, such as whether they are connected by an edge in a graph.\n", "\n"], "metadata": {"id": "LcP21cus1ccy"}}, {"cell_type": "markdown", "source": ["### The downside\n", "While this idea is very powerful, it comes with a major limitation. A GNN typically only implictly muiltiply the node vectors $H$ with the adjecency matrix. In practice this is implemented by a _sparse_ operation as we saw in the previous notebook. This means that the computational cost is $O(n d k)$, where $n$ is the number of nodes and $d$ the average (TODO, maximum?) degree and $k$ the dimensionality of the node vectors. \n", "\n", "To multiply with a dense matrix this instead becomes $O(n^2 k)$ and this quadratic scaling on the number of nodes in the input severely limits the application of this idea.\n", "\n", "Generating $A$ also scales quadratically with the number of nodes, since we need to apply the function to all pairs of node vectors, regardless of edges.\n", "\n", "This fact has not stopped this idea of becoming wildly successful in the domain of Natural Language Processing, and as long as the domain we're working has relatively small inputs (like organic molecules in medicinal chemistry) we can handle the quadratic scaling with input with brute force (a lot of computational capacity).\n", "\n", "\n", "\n"], "metadata": {"id": "6VcVGLTD1efV"}}, {"cell_type": "markdown", "source": ["### Implementing a Transformer\n", "\n", "Below is the implementation of the transformer. In aparticular the `BasicTransformerLayer` is where the main difference from a GNN is implemented. The `BasicTransformerEncoder` is very similar to the GNN encoder."], "metadata": {"id": "4TR-Nmns1lnq"}}, {"cell_type": "code", "metadata": {"id": "5RI0wbOQhTwm"}, "source": ["import math\n", "\n", "import torch\n", "from torch.nn import Module, Embedding, ModuleList, Linear, Sequential, ReLU, LayerNorm, Dropout\n", "from torch.nn.functional import softmax\n", "\n", "class BasicTransformerConfig:\n", "  def __init__(self, *, \n", "               d_model: int, \n", "               n_layers: int, \n", "               ffn_dim: int,\n", "               head_dim: int,\n", "               layer_normalization: bool = True,\n", "               dropout_rate: float = 0.1,\n", "               residual_connections: bool=True):\n", "    self.d_model = d_model\n", "    self.n_layers = n_layers\n", "    self.ffn_dim = ffn_dim\n", "    # Note that we introduce a new hyper parameter called *head_dim*, in\n", "    # Transformers we typically transform the \"node\" feature vectors to\n", "    # a lower dimensional space\n", "    self.head_dim = head_dim\n", "    self.layer_normalization = layer_normalization\n", "    self.dropout_rate = dropout_rate\n", "    self.residual_connections = residual_connections\n", "    \n", "class BasicTransformerLayer(Module):\n", "  def __init__(self, config):\n", "    super().__init__()    \n", "    self.config = config\n", "    self.input_dim = config.d_model\n", "    self.output_dim = config.d_model\n", "    self.ffn_dim = config.ffn_dim\n", "    self.head_dim = config.head_dim\n", "\n", "    # Transformers typically don't use mlps to create the neighbours and center\n", "    # embeddings, instead relying on just linear transformations\n", "    self.neighbour_transform = Linear(self.input_dim, self.head_dim, bias=False)\n", "    self.center_transform = Linear(self.input_dim, self.head_dim, bias=False)\n", "    \n", "    # The transformer uses layer normalization by default\n", "    self.attention_norm = LayerNorm(self.input_dim)\n", "    \n", "    self.output_transform = Sequential(Linear(self.input_dim, self.ffn_dim),\n", "                                       ReLU(), \n", "                                       Linear(self.ffn_dim, self.output_dim))\n", "                                       \n", "    self.output_norm = LayerNorm(self.output_dim)\n", "    self.dropout = Dropout(self.config.dropout_rate)\n", "   \n", "    self.scaling_factor = math.sqrt(self.input_dim)\n", "  \n", "  def attention_function(self, adjacency_matrix, center_node_features, \n", "                         neighbour_node_features, edge_features, node_mask, edge_mask):\n", "    #The standard Transformer just \n", "    # take the dot product between the center node and the neighbour nodes, \n", "    # scaled by the square root of the model dimension.\n", "    # To take the dot products between all center nodes and all neighbour nodes\n", "    # We perform an outer product by first transposing one of the matrices along\n", "    # the last two axises\n", "    # In the single-graph case the matrix multplication between \n", "    # a matrix with shape (n_nodes, head_dim) times (head_dim, n_nodes)\n", "    # gives the resulting matrix of shape (n_nodes, n_nodes), where each element\n", "    # is the dot product of the column for a node in one first with the row of a \n", "    # node in the other\n", "    attention_logits = torch.matmul(center_node_features, \n", "                                    neighbour_node_features.transpose(-1, -2))/self.scaling_factor\n", "\n", "    # The \"adjcency matrix\" of the transformer is actually using weighted means\n", "    # for the aggregation, and the way we achieve this is to make sure that the\n", "    # rows of the matrix are  normalized using softmax\n", "    # However, if we have a batch of graphs as inputs, we will have some \n", "    # \"positions\" in the node features which we should not include in our \n", "    # aggregation, and should therefore mask out in our attention matrix.\n", "    # If we did that after the softmax calculations, the rows would no longer\n", "    # add up to 1. Instead we do it before the softmax by essentially \n", "    # setting the masked values to va number so low it will end up as a \n", "    # 0 in the softmax output. The lowest value we can imagine is negative\n", "    # infinity, so let's use that. \n", "    # The goal is to mask out parts of the different batch attention_logits which\n", "    # are not part of the nodes, so essentially have a resulting matrix per batch\n", "    # example which looks something like\n", "    # [[   a,    b,    c, -inf, -inf],\n", "    #  [   d,    e,    f, -inf, -inf],\n", "    #  [   g,    h,    i, -inf, -inf],\n", "    #  [-inf, -inf, -inf, -inf, -inf],\n", "    #  [-inf, -inf, -inf, -inf, -inf],\n", "    # ]\n", "    # To do this we will first create a boolean mask which have True in the\n", "    # places we want to fill with '-inf'. We do this by using the node mask\n", "    # like in the GNN examples, doing something very much like an outer product\n", "    nodemask_2d = node_mask.unsqueeze(dim=-2) * node_mask.unsqueeze(dim=-1)\n", "    \n", "    # The nodemask has 1's where there are valid elements and 0's where there \n", "    # are none, we invert his and convert it to bool tensor\n", "    fill_mask = (1 - nodemask_2d).to(torch.bool)\n", "    \n", "    # We're now ready to 'mask out' the logits. Notice that masked_fill_ is \n", "    # in place\n", "    attention_logits.masked_fill_(fill_mask, float('-inf'))\n", "    attention_matrix = softmax(attention_logits, dim=-1)\n", "\n", "    # There will be rows of the smaller attention matrices which where filled\n", "    # completely with -inf values, these will now be rows of 'nan' values\n", "    # We perform a new fill of the attention matrix, but this time with 0s \n", "    attention_matrix = attention_matrix.masked_fill(fill_mask, 0.)\n", "    return attention_matrix\n", "\n", "\n", "  def forward(self, adjacency_matrix, node_features, edge_features, node_mask, edge_mask):\n", "    # In this basic Transformer layer we'll not use the edge features, and instead\n", "    # focus on the basic transformer formulation of this problem. This will pretty\n", "    # much treat the graph as just a node set. We should not expect this to \n", "    # be able to anything which relies on the graph structure\n", "    center_node_features = self.center_transform(node_features)\n", "    neighbour_node_features = self.neighbour_transform(node_features)\n", "\n", "    # The transformed node features are either a 3-tensor \n", "    # (batch_size, max_nodes, head_dim) or a matrix (n_nodes, head_dim) when\n", "    # it's a single graph. We'll make this code agnostic to that\n", "    # The goal now is to _compute_ an \"adjacency matrix\" using the node features\n", "    # This could be done by any function. We define a method on this class\n", "    # which is the attention function which has the purpose of giving us an\n", "    # \"adjacency matrix\"\n", "    attention_matrix = self.attention_function(adjacency_matrix, \n", "                                               center_node_features,\n", "                                               neighbour_node_features, \n", "                                               edge_features, \n", "                                               node_mask, \n", "                                               edge_mask)\n", "    \n", "    # Now we aggregate the neighbourhoods using the attention matrix (our computed \"adjacency matrix\")\n", "    # The transformer doesn't transform the node features at this stage, instead doing it in a separate MLP after residual connections and aggregation\n", "    aggregated_neighbourhoods = torch.matmul(attention_matrix, node_features)\n", "\n", "    # and mask the result    \n", "    masked_features = aggregated_neighbourhoods * node_mask.unsqueeze(dim=-1)\n", "    \n", "    # Followed by a dropout and layer normalization\n", "    masked_features = self.dropout(masked_features)\n", "    masked_features = self.attention_norm(masked_features)\n", "\n", "    # The transformer by default uses residual connections\n", "    if self.config.residual_connections:\n", "      masked_features = masked_features + node_features\n", "\n", "    # Now we apply the output transform as in the GNN\n", "    updated_node_features = self.output_transform(masked_features)\n", "\n", "    # Mask again\n", "    updated_node_features = updated_node_features * node_mask.unsqueeze(dim=-1)\n", "\n", "    # Followed by a dropout and normalization\n", "    updated_node_features = self.dropout(updated_node_features)\n", "    updated_node_features = self.output_norm(updated_node_features)\n", "\n", "    # And the resiudal connection from the input to the output MLP\n", "    if self.config.residual_connections:\n", "      updated_node_features = updated_node_features + masked_features\n", "\n", "    return updated_node_features\n", "\n", "\n", "class BasicTransformerEncoder(torch.nn.Module):\n", "  def __init__(self, *,\n", "               config: BasicTransformerConfig, \n", "               continuous_node_variables=None,\n", "               categorical_node_variables=None,\n", "               continuous_edge_variables=None,\n", "               categorical_edge_variables=None,\n", "               layer_type=BasicTransformerLayer):\n", "    super().__init__()\n", "\n", "    self.config = config\n", "    self.layer_type = layer_type\n", "\n", "    self.continuous_node_variables = continuous_node_variables\n", "    self.categorical_node_variables = categorical_node_variables\n", "    self.continuous_edge_variables = continuous_edge_variables\n", "    self.categorical_edge_variables = categorical_edge_variables\n", "\n", "    # We want the embeddings together with the continuous values to be of dimension d_model, therefore the allocate d_model - len(continuous_variables) as the embeddings dim\n", "    self.categorical_node_embeddings_dim = config.d_model - len(self.continuous_node_variables)\n", "    self.categorical_edge_embeddings_dim = config.d_model - len(self.continuous_edge_variables)\n", "\n", "    self.node_featurizer = FeatureCombiner(self.categorical_node_variables, \n", "                                           self.categorical_node_embeddings_dim)\n", "    self.edge_featurizer = FeatureCombiner(self.categorical_edge_variables, \n", "                                           self.categorical_edge_embeddings_dim)\n", "    \n", "    # Notice that we use the supplied layer type above when creating the graph\n", "    # layers. This allows us to easily change the kind of graph layers\n", "    # we use later on\n", "    self.graph_layers = ModuleList([layer_type(config) for l in range(config.n_layers)])\n", "    \n", "  def forward(self, batch):\n", "    # First order of business is to embed the node embeddings\n", "    node_mask = batch['nodes_mask']\n", "    batch_size, max_nodes = node_mask.shape\n", "    \n", "    continuous_node_features = batch['continuous_node_features']\n", "    categorical_node_features = batch['categorical_node_features']\n", "    node_features = self.node_featurizer(continuous_node_features, categorical_node_features)\n", "    masked_node_features = node_features * node_mask.unsqueeze(-1)\n", "    \n", "    continuous_edge_features = batch['continuous_edge_features']\n", "    categorical_edge_features = batch['categorical_edge_features']\n", "    edge_features = self.edge_featurizer(continuous_edge_features, categorical_edge_features)\n", "    edge_mask = batch['edge_mask']\n", "    masked_edge_features = edge_features * edge_mask.unsqueeze(-1)\n", "\n", "    # We have now embedded the node features, we'll propagate them through our \n", "    # graph layers\n", "    adjacency_matrix = batch['adjacency_matrices']\n", "    memory_state = masked_node_features\n", "    for l in self.graph_layers:\n", "      memory_state = l(adjacency_matrix, memory_state, masked_edge_features , node_mask, edge_mask)\n", "\n", "    return memory_state"], "execution_count": null, "outputs": []}, {"cell_type": "code", "metadata": {"id": "HpLUo3Jp662d"}, "source": ["class GraphPredictionNeuralNetwork(Module):\n", "  def __init__(self, encoder, prediction_head):\n", "    super().__init__()\n", "    self.encoder = encoder\n", "    self.prediction_head = prediction_head\n", "\n", "  def forward(self, batch):\n", "    encoded_graph = self.encoder(batch)\n", "    prediction = self.prediction_head(encoded_graph, batch['nodes_mask'])\n", "    return prediction"], "execution_count": null, "outputs": []}, {"cell_type": "code", "metadata": {"id": "TthCjmpFSwlo"}, "source": ["torch.manual_seed(1729)\n", "d_model = 16\n", "basic_encoder_config = BasicTransformerConfig(d_model=d_model, \n", "                                      n_layers=2, \n", "                                      ffn_dim=16,\n", "                                      head_dim=8,\n", "                                      layer_normalization=True,\n", "                                      dropout_rate=0.1,\n", "                                      residual_connections=True)\n", "basic_transformer = BasicTransformerEncoder(config=basic_encoder_config, \n", "                                            continuous_node_variables=dataset.continuous_node_variables,\n", "                                            categorical_node_variables=dataset.categorical_node_variables,\n", "                                            continuous_edge_variables=dataset.continuous_edge_variables,\n", "                                            categorical_edge_variables=dataset.categorical_edge_variables)\n", "\n", "head_config = GraphPredictionHeadConfig(d_model=d_model, ffn_dim=32, pooling_type='sum')\n", "prediction_head = GraphPredictionHead(input_dim=d_model, output_dim=1, config=head_config)\n", "\n", "model = GraphPredictionNeuralNetwork(basic_transformer, prediction_head)\n", "\n", "loss_fn = BCEWithLogitsLoss()\n", "trainer = Trainer(model=model, \n", "                  loss_fn=loss_fn, \n", "                  training_dataloader=training_dataloader,\n", "                  dev_dataloader=dev_dataloader)"], "execution_count": null, "outputs": []}, {"cell_type": "code", "metadata": {"colab": {"base_uri": "https://localhost:8080/", "height": 49, "referenced_widgets": ["774c06c7bd964097bfcd5da0d8f92ad7", "4a92071e447a47b6891976b05427f02b", "77c672d9c3a74252af4a5025a2b3c510", "7d61d91585c2457a98b8c023d49a355b", "bee8e57f2b514b41b727c62d4168ae9c", "24fc2e351c50480498431ff50e087cda", "790490b6222741b8bd85445f52122688", "0377cc6028f44378a51e1963e24a5bbc", "34746fd22b344e1d9ddc0d97096f3142", "8a720071283e4a06ac71e4e0405d1c43", "4ebafc33f3dc4510966240fdc6abc5e9", "a7fb300bbdc54b11979ae3426b5f9339", "81b2113c13cf4d10a1cc7e2dff35ff6b", "f934c2bc2b5248c0b2fa4b89a9259f91", "14d9b82397e8425284a916f2eba917db", "2922c94e28d0440d9ffaf4207e266c01", "ed8de72b2b2d48e3a004fff6d50a49d8", "cb2d0d02f96b45db9548ac74fd3f76a6", "cbaf326d5476495aa87eff290d8d3c83", "69e21a8411034cb8aa3d79b56278ec0f", "12045485a0984bc1ad701debc32bcf7d", "b22fa7cff11e4e33965f8851d8941755", "8b2917d1b3834705b3953fcf6f350181", "221bbaac227d44218702d6f55cc1c2c3", "46895fcb2ce34f609d4af0134565cfa7", "3fc4da4858f64150b3ef399b61610497", "eea6b99cf76143cf8898baa0c39c959a", "15ed0c48d71440a0b11491a4c8183c81", "a8bf9a4a8f38412db8df40123237e705", "3c464fed378643a6a27e90e2e7aef76f", "adf6834f5e4b4634ae35530c393326ed", "0f255c4c5106497fb5373a63699545fb", "daf4be53194c48b1a5d5ac0208810a60"]}, "id": "x_dXrOFY6cyB", "executionInfo": {"status": "ok", "timestamp": 1637727142782, "user_tz": -60, "elapsed": 9519, "user": {"displayName": "Erik Ylip\u00e4\u00e4", "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Ghv2ZnBVJIOqdwt_HBhCMHTtEbc2FUGzMgmKQEW7VE=s64", "userId": "10747062110949017925"}}, "outputId": "c200bb24-6c1d-470b-a3e4-6cff8985ad94"}, "source": ["trainer.train(1)"], "execution_count": null, "outputs": [{"output_type": "display_data", "data": {"application/vnd.jupyter.widget-view+json": {"model_id": "774c06c7bd964097bfcd5da0d8f92ad7", "version_minor": 0, "version_major": 2}, "text/plain": ["Epoch:   0%|          | 0/1 [00:00<?, ?it/s]"]}, "metadata": {}}, {"output_type": "display_data", "data": {"application/vnd.jupyter.widget-view+json": {"model_id": "a7fb300bbdc54b11979ae3426b5f9339", "version_minor": 0, "version_major": 2}, "text/plain": ["Training batch:   0%|          | 0/51 [00:00<?, ?it/s]"]}, "metadata": {}}, {"output_type": "display_data", "data": {"application/vnd.jupyter.widget-view+json": {"model_id": "8b2917d1b3834705b3953fcf6f350181", "version_minor": 0, "version_major": 2}, "text/plain": ["Dev batch:   0%|          | 0/7 [00:00<?, ?it/s]"]}, "metadata": {}}]}, {"cell_type": "markdown", "metadata": {"id": "Rh7jNy099kfo"}, "source": ["## Adding the graph structure to Transformers\n", "\n", "In basic GNNs, we take the graph strucuture into account by using the _given_ adjacency matrix. In the Transformer we've seen now, we _compute_ an _attention matrix_, a matrix which plays the same role as the adjacency matrix of the basic graph neural network.\n", "\n", "We can simply modify our _attention function_ of the Transformer to also take the adjacency matrix into account.\n", "\n", "This attention function could really be anything, but for now we'll stick with a very simple idea of modifying the logits of our scaled dot-product attention by adding a scalar to the logits if there is a 1 in the adjacency matrix for that place.\n", "\n", "If $a_{i,j}$ is the the value before the softmax of the _attention logit_ matrix for how much we should include information from node $j$ when aggregating for node $i$, we define a function:\n", "\n", "$$\n", "  a_{i,j} = f(\\mathbf{x_i}, \\mathbf{x_j}) = \n", "  \\frac{<\\mathbf{x_i}, \\mathbf{x_j}>}{\\sqrt{\\text{d_model}}} + w\\mathbf{1}_{\\{i, j\\} \\in E}\n", "$$\n", "\n", "Here ${1}_{\\{i, j\\} \\in E}$ is the indicator function taking the value $1$ if there is an edge between  $i$ and $j$ (in other words, the $i,j$ entry of the adjacency matrix).\n", "\n", "We also add a learnable scalar $w$ which the network can learn to set to change what influence the prescence of an edge has on the attention score.\n"]}, {"cell_type": "code", "metadata": {"id": "p4vWuOuc-v_c"}, "source": ["from torch.nn import Parameter\n", "class AdjacencyTransformerLayer(BasicTransformerLayer):\n", "  def __init__(self, *args, **kwargs):\n", "    super().__init__(*args, **kwargs)\n", "    self.adjacency_weight = Parameter(torch.tensor(1.))\n", "\n", "  def attention_function(self, adjacency_matrix, center_node_features, \n", "                         neighbour_node_features, edge_features, node_mask, edge_mask):\n", "    # We still take the \n", "    dot_product_logits = torch.matmul(center_node_features, \n", "                                                neighbour_node_features.transpose(-1, -2))/self.scaling_factor\n", "\n", "    # The adjacency_matrix is scaled by a parameter so that the network can\n", "    # learn to adjust the influence of edge presence\n", "    adjacency_logits = self.adjacency_weight*adjacency_matrix\n", "    attention_logits = dot_product_logits + adjacency_logits\n", "\n", "    nodemask_2d = node_mask.unsqueeze(dim=-2) * node_mask.unsqueeze(dim=-1)\n", "    fill_mask = (1 - nodemask_2d).to(torch.bool)\n", "    attention_logits.masked_fill_(fill_mask, float('-inf'))\n", "    attention_matrix = softmax(attention_logits, dim=-1)\n", "\n", "    # There will be rows of the smaller attention matrices which where filled\n", "    # completely with -inf values, these will now be rows of 'nan' values\n", "    # We perform a new fill of the attention matrix, but this time with 0s \n", "    attention_matrix = attention_matrix.masked_fill(fill_mask, 0.)\n", "    return attention_matrix"], "execution_count": null, "outputs": []}, {"cell_type": "code", "metadata": {"id": "gvOUPADDwiNr"}, "source": ["torch.manual_seed(1729)\n", "d_model = 16\n", "basic_encoder_config = BasicTransformerConfig(d_model=d_model, \n", "                                      n_layers=2, \n", "                                      ffn_dim=16,\n", "                                      head_dim=8,\n", "                                      layer_normalization=True,\n", "                                      dropout_rate=0.1,\n", "                                      residual_connections=True)\n", "basic_transformer = BasicTransformerEncoder(config=basic_encoder_config, \n", "                                            continuous_node_variables=dataset.continuous_node_variables,\n", "                                            categorical_node_variables=dataset.categorical_node_variables,\n", "                                            continuous_edge_variables=dataset.continuous_edge_variables,\n", "                                            categorical_edge_variables=dataset.categorical_edge_variables,\n", "                                            layer_type=AdjacencyTransformerLayer)\n", "\n", "config = GraphPredictionHeadConfig(d_model=32, ffn_dim=32, pooling_type='sum')\n", "prediction_head = GraphPredictionHead(input_dim=head_config.d_model, output_dim=1, config=head_config)\n", "\n", "model = GraphPredictionNeuralNetwork(basic_transformer, prediction_head)\n", "\n", "loss_fn = BCEWithLogitsLoss()\n", "trainer = Trainer(model=model, \n", "                  loss_fn=loss_fn, \n", "                  training_dataloader=training_dataloader,\n", "                  dev_dataloader=dev_dataloader)"], "execution_count": null, "outputs": []}, {"cell_type": "code", "metadata": {"colab": {"base_uri": "https://localhost:8080/", "height": 49, "referenced_widgets": ["62b8e93c07c2466d9a48127c951b9bcb", "1aa7e8ee389a4082b4b11ee33458ae1f", "8c315de06a0241b29d2b1399dc3deab0", "30a7cac37cdf4173815d383f6f272878", "da0985f84aca4bf1a7ba53c439a15ad4", "31d924383cd84ea6b9a1f6b64b7f4d0f", "81239df994d94c8ca56a4025e729a2e5", "d7c5a634a3af488181fa818b27a000d2", "333104008254448eb156888d41d8d624", "3a7a358260a84fd4adc9ab27284290ea", "9e0d5bd679a94195b1e7fb694a4c33d8", "3f12595a485641ae92f0dd25606d7c55", "cf273512695542efa930221f2bb6120e", "ffb8c3562a5f4de1b160f741d48456fd", "a4f69b9300d74e38a7ce678e9d386943", "cd16db78654d47e398be679a348b6057", "f5d4e5469f814431b589c8c95ebbc9d2", "43b35d4108c5438cb205184cf0be28ce", "f7d9f93e38d24c76bd8ba76072c2d336", "4e6908e021d640af8ea8ad9af0e5776d", "10e59f31868a49b48079586996cc3344", "d0da654b142a46e2ad1b0418070c12a9", "864697da89b942988cface87062cb27b", "ffdc50e84f5440d89a14189e2320f98a", "f7e34eb0f5414b98a6fbd81bade96755", "d01335ce5ca54cf3886e97a0970cf72e", "66897b69905c4a36a76b0caaa41a7041", "c6efba4b0cb648eab7e0b349e68ce5ec", "530a2896a6ac4d83ac198a0a997171ab", "b831d09fea5c4820891761d6d7edbe2d", "e07d9ce60ac840ee91ad4d1a76334de4", "738a9cf4e43441259f142501a9b9ae1d", "3cb83d701e5e481693b3620fd5cabd16"]}, "id": "Jb7hwVaeQrkW", "executionInfo": {"status": "ok", "timestamp": 1634560956073, "user_tz": -120, "elapsed": 8744, "user": {"displayName": "Erik Ylip\u00e4\u00e4", "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Ghv2ZnBVJIOqdwt_HBhCMHTtEbc2FUGzMgmKQEW7VE=s64", "userId": "10747062110949017925"}}, "outputId": "2abfc7bb-0bda-448e-df2c-bcbc81b19968"}, "source": ["trainer.train(1)"], "execution_count": null, "outputs": [{"output_type": "display_data", "data": {"application/vnd.jupyter.widget-view+json": {"model_id": "62b8e93c07c2466d9a48127c951b9bcb", "version_minor": 0, "version_major": 2}, "text/plain": ["Epoch:   0%|          | 0/1 [00:00<?, ?it/s]"]}, "metadata": {}}, {"output_type": "display_data", "data": {"application/vnd.jupyter.widget-view+json": {"model_id": "3f12595a485641ae92f0dd25606d7c55", "version_minor": 0, "version_major": 2}, "text/plain": ["Training batch:   0%|          | 0/51 [00:00<?, ?it/s]"]}, "metadata": {}}, {"output_type": "display_data", "data": {"application/vnd.jupyter.widget-view+json": {"model_id": "864697da89b942988cface87062cb27b", "version_minor": 0, "version_major": 2}, "text/plain": ["Dev batch:   0%|          | 0/7 [00:00<?, ?it/s]"]}, "metadata": {}}]}, {"cell_type": "markdown", "metadata": {"id": "50EsNUSG1tYR"}, "source": ["## Edge features in the attention\n", "\n", "\n", "In the previous example, we only used the _adjacency matrix_ to influence the attention function. Now we'll extend this to actually include edge features as well. \n", "\n", "To do this we simply extend this notion of using a function to compute the values of the attention matrix, in particular we will use function which takes the feature nodes for the center and neighbour node as well as the edge feature vector between these nodes. For nodes which have no feature vector, the zero-vector will be used.\n", "\n", "This function will be implemented by a simple 2 layer MLP. And we choose to implement it as below\n", "\n", "$$\n", "a_{i,j} = f(\\mathbf{x_i}, \\mathbf{x_j}, \\mathbf{x_{e_{i,j}}}) = \n", "  W_2 \\sigma(W_1 (\\mathbf{x_i} + \\mathbf{x_j} + \\mathbf{x_{e_{i,j}}}) + \\mathbf{b_1}) + \\mathbf{b_2}\n", "$$\n", "\n", "You can see that we apply the neural network on the sum the vectors of node features and edge features. This will make this function permutation invariant of its inputs (i.e. $f(\\mathbf{x_i}, \\mathbf{x_j}, \\mathbf{x_{e_{i,j}}}) = f( \\mathbf{x_j}, \\mathbf{x_{e_{i,j}}}, \\mathbf{x_i},)$. If this is not desired, concatenation can be used instead but makes the implementation a bit more complex.\n", "\n"]}, {"cell_type": "code", "metadata": {"id": "Raj1qXSA1qVk"}, "source": ["class EdgeAttributesAttentionTransformerLayer(BasicTransformerLayer):\n", "  def __init__(self, *args, **kwargs):\n", "    super().__init__(*args, **kwargs)\n", "\n", "    # We're going to sum our center and neighbour vectors to the edge feature\n", "    # vector, so have to be mindful of the dimensionality\n", "    self.center_transform = Linear(self.input_dim, self.input_dim)\n", "    self.neighbour_transform = Linear(self.input_dim, self.input_dim)\n", "    self.attention_score_function = Sequential(Linear(self.input_dim, \n", "                                                      self.config.ffn_dim), \n", "                                               ReLU(), \n", "                                               Linear(self.config.ffn_dim, \n", "                                                      1))\n", "\n", "    \n", "  def attention_function(self, adjacency_matrix, center_node_features, \n", "                         neighbour_node_features, edge_features, \n", "                         node_mask, edge_mask):\n", "    # Our goal is to first build up the input tensor to our simple MLP. \n", "    # This will be a tensor of shape \n", "    # (*leading_dimension, n_nodes, n_nodes, feature_dim)\n", "    # The edge_feature's tensor already has this shape, and we've made sure\n", "    # the edge features on that tensor is already zeroed for places where\n", "    # there are no edges.\n", "    # We want each element [...,i,j,:] of this matrix to be \n", "    # center_node_features[i] + neighbour_node_features[j] + edge_features[i,j]\n", "    # We achieve this by broadcasting once again, the center node features\n", "    # are broadcasted along the -3'rd axis and the neighbour node features\n", "    # along the -2'nd axis which gives us the desired result\n", "    # This is one of the most annyoing things with the frameworks we use, \n", "    # having to be very conscious about the order of axises\n", "    # unsqueeze(-2) broadcasts along the \"row\" of this batch of edge feature \"matrices\"\n", "    attention_score_input = edge_features + center_node_features.unsqueeze(dim=-2) \n", "    # unsqueeze(-3) broadcasts along the \"columns\" if the 3-tensor\n", "    attention_score_input = attention_score_input + neighbour_node_features.unsqueeze(dim=-3)\n", "    \n", "    attention_logits = self.attention_score_function(attention_score_input).squeeze()\n", "    \n", "    # We need to perform the same masking as before\n", "    nodemask_2d = node_mask.unsqueeze(dim=-2) * node_mask.unsqueeze(dim=-1)\n", "    fill_mask = (1 - nodemask_2d).to(torch.bool)\n", "    \n", "    attention_logits.masked_fill_(fill_mask, float('-inf'))\n", "    attention_matrix = softmax(attention_logits, dim=-1)\n", "    attention_matrix = attention_matrix.masked_fill(fill_mask, 0.)\n", "    return attention_matrix"], "execution_count": null, "outputs": []}, {"cell_type": "code", "metadata": {"id": "3_F13BBCUK9G"}, "source": ["torch.manual_seed(1729)\n", "d_model = 16\n", "basic_encoder_config = BasicTransformerConfig(d_model=d_model, \n", "                                      n_layers=2, \n", "                                      ffn_dim=16,\n", "                                      head_dim=8,\n", "                                      layer_normalization=True,\n", "                                      dropout_rate=0.1,\n", "                                      residual_connections=True)\n", "basic_transformer = BasicTransformerEncoder(config=basic_encoder_config, \n", "                                            continuous_node_variables=dataset.continuous_node_variables,\n", "                                            categorical_node_variables=dataset.categorical_node_variables,\n", "                                            continuous_edge_variables=dataset.continuous_edge_variables,\n", "                                            categorical_edge_variables=dataset.categorical_edge_variables,\n", "                                            layer_type=EdgeAttributesAttentionTransformerLayer)\n", "\n", "head_config = GraphPredictionHeadConfig(d_model=d_model, ffn_dim=32, pooling_type='sum')\n", "prediction_head = GraphPredictionHead(input_dim=d_model, output_dim=1, config=head_config)\n", "\n", "model = GraphPredictionNeuralNetwork(basic_transformer, prediction_head)\n", "\n", "loss_fn = BCEWithLogitsLoss()\n", "trainer = Trainer(model=model, \n", "                  loss_fn=loss_fn, \n", "                  training_dataloader=training_dataloader,\n", "                  dev_dataloader=dev_dataloader)"], "execution_count": null, "outputs": []}, {"cell_type": "code", "metadata": {"colab": {"base_uri": "https://localhost:8080/", "height": 49, "referenced_widgets": ["d5b7af95c5ca4d6fb9c839aad3a773eb", "ab5e576a7df342f8a5ff66709c51fce3", "e3707155c6214b0faeff482ed400bab7", "79ee8553e1f24d0e9c0e05ba0ac9e186", "88a935d2592f451c86a6ddd2da8d5ccd", "80df814116ea4531bb19b54712e48eb1", "cc7a7560ebf042eb9497ec2da1ebf315", "16f91e3c8e8d4805998bee298df76815", "3385828af4dd4f71ad70ca5912cb413c", "c48f31b1a7cb4abe847e8603a28e0e07", "4b79b90f0b6640118fbaa1a7c3609260", "e933598f725846619d68532a266bf1c6", "f909b158c85c4d50a160cc6b90f402c5", "fcda8e47828649fda5ba3ee01f9fe6d6", "967339bec0df44cda7ae475ba3b1bdcb", "8375c9005c67408fbf63df06d7b8b700", "d87fdea0bedc4f848c0891f4b6f9bbfb", "81488436a18c4ed9a55ac6f5ee9944fa", "b3eaaa1f41af4421a5fa74e12e5bebb2", "f73da2dbef3c4452b8be5f7b418cf1c2", "c599c2a465834fa7906c26468de2bf73", "b1b27bc6348b4edfb48136be7c720a72", "c53529ac23834684b2d8d5b7ce839208", "61f0a021c2504669932374f0a0ebb4fb", "b421d9ff3d054375929a572e2577cdf7", "f0e0fa105ab24c79afde5d0e39773e08", "789e9c7e3bdc4c059f451c1cf0e5fc78", "7946866077b142f8a88deda3adc985cd", "2ac38fde7dfc43f88b4e22a9d4702102", "6bb9f0cdba144bb0b7656a985b6d79c8", "91c09713b6ff49fd9860eba89cdb07f3", "7a7d00e483764314b8cfb3297b79eb81", "63c06fc189f741c6bea1359181f7c547"]}, "id": "ggwccIO-ho03", "executionInfo": {"status": "ok", "timestamp": 1637727237877, "user_tz": -60, "elapsed": 9274, "user": {"displayName": "Erik Ylip\u00e4\u00e4", "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Ghv2ZnBVJIOqdwt_HBhCMHTtEbc2FUGzMgmKQEW7VE=s64", "userId": "10747062110949017925"}}, "outputId": "c6df1693-d7a8-4814-9761-c05c1c8d3a5b"}, "source": ["trainer.train(1)"], "execution_count": null, "outputs": [{"output_type": "display_data", "data": {"application/vnd.jupyter.widget-view+json": {"model_id": "d5b7af95c5ca4d6fb9c839aad3a773eb", "version_minor": 0, "version_major": 2}, "text/plain": ["Epoch:   0%|          | 0/1 [00:00<?, ?it/s]"]}, "metadata": {}}, {"output_type": "display_data", "data": {"application/vnd.jupyter.widget-view+json": {"model_id": "e933598f725846619d68532a266bf1c6", "version_minor": 0, "version_major": 2}, "text/plain": ["Training batch:   0%|          | 0/51 [00:00<?, ?it/s]"]}, "metadata": {}}, {"output_type": "display_data", "data": {"application/vnd.jupyter.widget-view+json": {"model_id": "c53529ac23834684b2d8d5b7ce839208", "version_minor": 0, "version_major": 2}, "text/plain": ["Dev batch:   0%|          | 0/7 [00:00<?, ?it/s]"]}, "metadata": {}}]}, {"cell_type": "markdown", "metadata": {"id": "-WSLaNUJz1Qq"}, "source": ["## Edge features in the transformations\n", "\n", "While the basic Transformer can include pairwise information (such as the adjacency matrix) in its attention function, this is only used to decide what node vectors to aggregate, i.e. the values of the attention matrix (our dynamic adjacency matrix).\n", "\n", "In the GNN we looked at in the last notebook, edge feature vectors where integrated into the _message_ \"passed\" from the neighbour nodes, and could be used by the MLP parts of the network to learn a combined representation of the node features and edge features and eventually directly part of the output.\n", "\n", "We can extend the Transformer architecture to also do this just like we did in the example on GNNs. We will still use the underlying idea of _computing_ an adjacency matrix to have a dynamic aggregation function, but instead making the aggregated neighbourhood be contexually different based on what node we're currently aggregating for."]}, {"cell_type": "code", "metadata": {"id": "fYVXTC7yz1Qu"}, "source": ["from torch.nn import Parameter\n", "class EdgeAttributesTransformerLayer(EdgeAttributesAttentionTransformerLayer):\n", "  def __init__(self, *args, **kwargs):\n", "    super().__init__(*args, **kwargs)\n", "\n", "  def forward(self, adjacency_matrix, node_features, edge_features, node_mask, edge_mask):\n", "    center_node_features = self.center_transform(node_features)\n", "    neighbour_node_features = self.center_transform(node_features)\n", "    attention_matrix = self.attention_function(adjacency_matrix, \n", "                                               center_node_features,\n", "                                               neighbour_node_features, \n", "                                               edge_features, \n", "                                               node_mask, \n", "                                               edge_mask)\n", "    # Just like when we computed the attention scores using the edge feature \n", "    # vectors we also want to create context-dependent neighbourhoods now\n", "    # We do that by creating the full pairwise tensor of shape\n", "    # (*leading_dimensions, n_nodes, n_nodes, num_features). We use the same\n", "    # procedure with broadcasting in the \n", "    # EdgeAttributesAttentionTransformerLayer attention_function above\n", "    # unsqueeze(-2) broadcasts along the \"row\" of this batch of edge feature \"matrices\"\n", "    context_dependent_features = edge_features + center_node_features.unsqueeze(dim=-2) \n", "    # unsqueeze(-3) broadcasts along the \"columns\" if the 3-tensor\n", "    context_dependent_features = context_dependent_features + neighbour_node_features.unsqueeze(dim=-3)\n", "    \n", "    # Now the aggregation becomes a bit more tricky. We did this in the GNN layer\n", "    # which used edge features as well, essentially explicitly performing what \n", "    # used a matrix multiplaction to do previously: broadcast the attention matrix\n", "    # over the feature dimension: multiplying a feature vector at position i,j \n", "    # with the value in the attention matrix at position i,j\n", "    attended_features = context_dependent_features * attention_matrix.unsqueeze(dim=-1)\n", "\n", "    # Now the aggregation is performed by summing these attended features along \n", "    # the \"rows\", i.e. reducing away the \"column\" axis which is dim=-2\n", "    aggregated_neighbourhoods = attended_features.sum(dim=-2)  \n", "\n", "    # Now maske the result as before\n", "    masked_features = aggregated_neighbourhoods * node_mask.unsqueeze(dim=-1)\n", "    \n", "    # Followed by a dropout, layer normalization and residual sum\n", "    masked_features = self.dropout(masked_features)\n", "    masked_features = self.attention_norm(masked_features)\n", "    if self.config.residual_connections:\n", "      masked_features = masked_features + node_features\n", "\n", "    # Transform the features with our MLP\n", "    updated_node_features = self.output_transform(masked_features)\n", "\n", "    updated_node_features = updated_node_features * node_mask.unsqueeze(dim=-1)\n", "\n", "    # Followed by a dropout and normalization\n", "    updated_node_features = self.dropout(updated_node_features)\n", "    updated_node_features = self.output_norm(updated_node_features)\n", "\n", "    # And the resiudal connection from the input to the output MLP\n", "    if self.config.residual_connections:\n", "      updated_node_features = updated_node_features + masked_features\n", "\n", "    return updated_node_features\n", "\n", "  "], "execution_count": null, "outputs": []}, {"cell_type": "code", "metadata": {"id": "xx66k82YUwog"}, "source": ["torch.manual_seed(1729)\n", "d_model = 16\n", "basic_encoder_config = BasicTransformerConfig(d_model=d_model, \n", "                                      n_layers=2, \n", "                                      ffn_dim=16,\n", "                                      head_dim=8,\n", "                                      layer_normalization=True,\n", "                                      dropout_rate=0.1,\n", "                                      residual_connections=True)\n", "basic_transformer = BasicTransformerEncoder(config=basic_encoder_config, \n", "                                            continuous_node_variables=dataset.continuous_node_variables,\n", "                                            categorical_node_variables=dataset.categorical_node_variables,\n", "                                            continuous_edge_variables=dataset.continuous_edge_variables,\n", "                                            categorical_edge_variables=dataset.categorical_edge_variables,\n", "                                            layer_type=EdgeAttributesTransformerLayer)\n", "\n", "head_config = GraphPredictionHeadConfig(d_model=d_model, ffn_dim=32, pooling_type='sum')\n", "prediction_head = GraphPredictionHead(input_dim=d_model, output_dim=1, config=head_config)\n", "\n", "model = GraphPredictionNeuralNetwork(basic_transformer, prediction_head)\n", "\n", "loss_fn = BCEWithLogitsLoss()\n", "trainer = Trainer(model=model, \n", "                  loss_fn=loss_fn, \n", "                  training_dataloader=training_dataloader,\n", "                  dev_dataloader=dev_dataloader)"], "execution_count": null, "outputs": []}, {"cell_type": "code", "metadata": {"colab": {"base_uri": "https://localhost:8080/", "height": 49, "referenced_widgets": ["3462cf2081354ce5b87ad6c188a73107", "e3b4376e60bb4c338e9a6d29e9df5062", "8a8c0dddde6d49bda771373ce647e339", "41589d8a9be048fbbc222646fd6a052b", "38a8f8a01d5f41149fee8291d1d57bba", "3f26da0489674fdf8217bc93ce3c4b9b", "1d6a0e3edbc24d84a2fa31db739b479e", "4a05a625efda48c4b644d77653874a3f", "90247bc790124576832f2757993de070", "fed1d6d3efa046e7aaebaa194484d6e0", "8dfc96154aad4418bc4cf8319a83b453", "a56e8722bc054ec8a8a3f86623f4d893", "6bcd5f447038417dadece7b1f47d98b0", "590fbea74ca649858871613de1b14992", "98c711ebc59e454f99f2f20b1396ff30", "f65e370e9c7e46d8a65c1a3cc8cd6709", "d0621b1c57a647a087add556d720de93", "547bedfce4444044986a62daa9a0f18e", "6056253396ba498e8a413a9abff9a320", "578328673a2d450dab4f1b7be6803c13", "05dd8e9068df4eb99425e589a06f5210", "b1260e4792b946f7988d48b4ec84c03c", "6c7cabdf6e4543ddb2e4606b0d6374a7", "f991307020784eecb33836f59e4c2542", "ce3b86f2be4e40de944299ed356fbddf", "159f503364b54702bdd1f252c7342b23", "776a8a4dbafb44f5a6cd86054368b06d", "5ebda6b96e3248caab7a09c422f764e3", "2c575b90d0cc4862903dfdc9d9cb74ae", "797c6be9c6884ec487f2b5ebcd21d176", "15d79eb85d394a22bf7043cba089c9c4", "8f270a53c07b415183b39c24d1f782ed", "0e2a7867f2264f12acae96b1f1e11881"]}, "id": "tHJ-C9L0z1Qv", "executionInfo": {"status": "ok", "timestamp": 1637727281864, "user_tz": -60, "elapsed": 9249, "user": {"displayName": "Erik Ylip\u00e4\u00e4", "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Ghv2ZnBVJIOqdwt_HBhCMHTtEbc2FUGzMgmKQEW7VE=s64", "userId": "10747062110949017925"}}, "outputId": "85c47311-0b07-4133-9fd2-86680c5ba4ce"}, "source": ["trainer.train(1)"], "execution_count": null, "outputs": [{"output_type": "display_data", "data": {"application/vnd.jupyter.widget-view+json": {"model_id": "3462cf2081354ce5b87ad6c188a73107", "version_minor": 0, "version_major": 2}, "text/plain": ["Epoch:   0%|          | 0/1 [00:00<?, ?it/s]"]}, "metadata": {}}, {"output_type": "display_data", "data": {"application/vnd.jupyter.widget-view+json": {"model_id": "a56e8722bc054ec8a8a3f86623f4d893", "version_minor": 0, "version_major": 2}, "text/plain": ["Training batch:   0%|          | 0/51 [00:00<?, ?it/s]"]}, "metadata": {}}, {"output_type": "display_data", "data": {"application/vnd.jupyter.widget-view+json": {"model_id": "6c7cabdf6e4543ddb2e4606b0d6374a7", "version_minor": 0, "version_major": 2}, "text/plain": ["Dev batch:   0%|          | 0/7 [00:00<?, ?it/s]"]}, "metadata": {}}]}, {"cell_type": "markdown", "metadata": {"id": "zRN0Hpp1opTA"}, "source": ["## Pairwise features\n", "In the Graph Neural Network framework, we tend to focus on node attributes and edge attributes. With the Transformer framework we have instead an architecture which is based on all pairwise interaction between nodes. This allows us to extend the idea of what _edge features_ could be.\n", "\n", "Instead of only specifying features for pairs of nodes which have an edge, we can think of features between any pair of nodes. \n", "This can be used to give the network information about node relationships which it struggles to learn, but can easily compute with regular algorithms, such as distance between nodes if they are embedded in an euclidean space, or the shortest path between them if they are in a graph.\n", "\n", "This latter idea is the idea of a _path-augmented graph transformer_ [Chen Benson et al. \"Path-augmented graph transformer network.\"](https://arxiv.org/abs/1905.12712).\n", "\n", "We can use the architecture we've already defined above but allow our _edge features_ to actually be _pairwise features_. Instead of only having features for pairs of nodes which are directly connected to the graph, we introduce features between any pair of nodes.\n", "\n", "In this case the pairwise features will contain information about the edges along the shortest path of our graph, to at most `max_path_length` steps (longer paths are just truncated).\n", "\n", "### Representing the path information\n", "In the graph, the shortest path is defined by the edges between two nodes. A natural way of representing the path is then a sequence of the edge features along this shortest path. \n", "\n", "To handle the sequence information of these edges we could aggregate the path information using a sequence model such as a Recurrent Neural Network or a Transformer. In this example we make things a bit simpler and just represent the path features as separate categorical variables. \n", "We'll add these variables between all pairs of nodes.\n", "\n", "Since we have to decide on exactly what categorical variables to use when defining the network architecture, we decide on some max length $k$ and only take edge features along the path up to that number."]}, {"cell_type": "code", "metadata": {"id": "t3gjSO8ek_G_"}, "source": ["max_path_length = 3  # This is the k we limit our path lengths to"], "execution_count": null, "outputs": []}, {"cell_type": "code", "metadata": {"id": "X6eS13bcjeCI"}, "source": ["from itertools import combinations\n", "from rdkit.Chem.rdmolops import GetShortestPath\n", "from itertools import combinations\n", "from tqdm.notebook import tqdm\n", "\n", "\n", "PAIRWISE_FEATURES = []\n", "\n", "BOND_FEATURES = {'type_feature': TYPE_FEATURE, \n", "                 'direction_feature': DIRECTION_FEATURE, \n", "                 'aromatic_feature': AROMATIC_FEATURE, \n", "                 'stereo_feature': STEREO_FEATURE}\n", "\n", "PAIRWISE_FEATURES.extend(BOND_FEATURES.values())\n", "\n", "# We careate a copy of the bond features for each path step\n", "PATH_FEATURES = []\n", "for i in range(max_path_length):\n", "  path_vars = {}\n", "  for feature_kw, var in BOND_FEATURES.items():\n", "    name = var.name\n", "    path_var_name = f\"{name}_p{i}\"\n", "    if isinstance(var, ContinuousVariable):\n", "      path_var = ContinuousVariable(path_var_name)\n", "    if isinstance(var, CategoricalVariable):\n", "      path_var_values = var.values\n", "      path_var = CategoricalVariable(path_var_name, path_var_values, add_null_value=False)\n", "    path_vars[feature_kw] = path_var\n", "    PAIRWISE_FEATURES.append(path_var)\n", "  PATH_FEATURES.append(path_vars)\n", "\n", "\n", "def get_shortest_paths_bond_features(rd_bond,\n", "                      *,\n", "                      type_feature,\n", "                      direction_feature,\n", "                      aromatic_feature,\n", "                      stereo_feature):\n", "  \n", "  if rd_bond is not None:\n", "    bond_type = str(rd_bond.GetBondType())\n", "    bond_stereo_info = str(rd_bond.GetStereo())\n", "    bond_direction = str(rd_bond.GetBondDir())\n", "    is_aromatic = rd_bond.GetIsAromatic()\n", "  else:\n", "    bond_type = None\n", "    bond_stereo_info = None\n", "    bond_direction = None\n", "    is_aromatic = None\n", "\n", "  return {type_feature: bond_type,\n", "          direction_feature: bond_direction,\n", "          aromatic_feature: is_aromatic,\n", "          stereo_feature: bond_stereo_info}\n", "\n", "\n", "def get_pairwise_features(rd_mol, rd_atom_a, rd_atom_b):\n", "  pairwise_features = {}\n", "  # First we create the features for the bond (or missing such) between\n", "  # the two atoms\n", "  bond = rd_mol.GetBondBetweenAtoms(rd_atom_a.GetIdx(), rd_atom_b.GetIdx())\n", "  bond_features = get_shortest_paths_bond_features(bond, **BOND_FEATURES)\n", "  \n", "  pairwise_features.update(bond_features)\n", "  # Now we create bond features for the path between rd_atom_a and rd_atom_b\n", "  # We iterate over atoms of the shortest path up till max_path_length\n", "  # If the shortest path is shorter than max_path_length, we add None-valued\n", "  # features for the remaining ones\n", "  shortest_path = GetShortestPath(rd_mol, rd_atom_a.GetIdx(), rd_atom_b.GetIdx())\n", "  for i in range(max_path_length):\n", "    path_bond_variables = PATH_FEATURES[i]\n", "    if i < (len(shortest_path) - 1):\n", "      a, b = shortest_path[i], shortest_path[i+1]\n", "      path_bond = rd_mol.GetBondBetweenAtoms(a, b)\n", "    else:\n", "      path_bond = None\n", "    path_bond_features = get_shortest_paths_bond_features(path_bond, **path_bond_variables)\n", "    pairwise_features.update(path_bond_features)\n", "\n", "  \n", "  return pairwise_features"], "execution_count": null, "outputs": []}, {"cell_type": "code", "metadata": {"colab": {"base_uri": "https://localhost:8080/"}, "id": "Iq8NsQvusoZN", "executionInfo": {"status": "ok", "timestamp": 1637727324639, "user_tz": -60, "elapsed": 429, "user": {"displayName": "Erik Ylip\u00e4\u00e4", "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Ghv2ZnBVJIOqdwt_HBhCMHTtEbc2FUGzMgmKQEW7VE=s64", "userId": "10747062110949017925"}}, "outputId": "bed26ba9-806e-4eb0-e52d-b52d9936c9ad"}, "source": ["rd_mol = MolFromSmiles('CCCCCC')\n", "atom_a = rd_mol.GetAtomWithIdx(0)\n", "atom_b = rd_mol.GetAtomWithIdx(1)\n", "get_pairwise_features(rd_mol, atom_a, atom_b)"], "execution_count": null, "outputs": [{"output_type": "execute_result", "data": {"text/plain": ["{<CategoricalVariable: bond_direction>: 'NONE',\n", " <CategoricalVariable: bond_direction_p0>: 'NONE',\n", " <CategoricalVariable: bond_direction_p1>: None,\n", " <CategoricalVariable: bond_direction_p2>: None,\n", " <CategoricalVariable: bond_stereo>: 'STEREONONE',\n", " <CategoricalVariable: bond_stereo_p0>: 'STEREONONE',\n", " <CategoricalVariable: bond_stereo_p1>: None,\n", " <CategoricalVariable: bond_stereo_p2>: None,\n", " <CategoricalVariable: bond_type>: 'SINGLE',\n", " <CategoricalVariable: bond_type_p0>: 'SINGLE',\n", " <CategoricalVariable: bond_type_p1>: None,\n", " <CategoricalVariable: bond_type_p2>: None,\n", " <CategoricalVariable: is_aromatic>: False,\n", " <CategoricalVariable: is_aromatic_p0>: False,\n", " <CategoricalVariable: is_aromatic_p1>: None,\n", " <CategoricalVariable: is_aromatic_p2>: None}"]}, "metadata": {}, "execution_count": 43}]}, {"cell_type": "code", "metadata": {"colab": {"base_uri": "https://localhost:8080/"}, "id": "ldG7Zv_SVg-Q", "executionInfo": {"status": "ok", "timestamp": 1637727328635, "user_tz": -60, "elapsed": 413, "user": {"displayName": "Erik Ylip\u00e4\u00e4", "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Ghv2ZnBVJIOqdwt_HBhCMHTtEbc2FUGzMgmKQEW7VE=s64", "userId": "10747062110949017925"}}, "outputId": "c8371bc2-1b81-4995-bd1c-4ecb1f87e791"}, "source": ["atom_a = rd_mol.GetAtomWithIdx(0)\n", "atom_b = rd_mol.GetAtomWithIdx(5)\n", "get_pairwise_features(rd_mol, atom_a, atom_b)"], "execution_count": null, "outputs": [{"output_type": "execute_result", "data": {"text/plain": ["{<CategoricalVariable: bond_direction>: None,\n", " <CategoricalVariable: bond_direction_p0>: 'NONE',\n", " <CategoricalVariable: bond_direction_p1>: 'NONE',\n", " <CategoricalVariable: bond_direction_p2>: 'NONE',\n", " <CategoricalVariable: bond_stereo>: None,\n", " <CategoricalVariable: bond_stereo_p0>: 'STEREONONE',\n", " <CategoricalVariable: bond_stereo_p1>: 'STEREONONE',\n", " <CategoricalVariable: bond_stereo_p2>: 'STEREONONE',\n", " <CategoricalVariable: bond_type>: None,\n", " <CategoricalVariable: bond_type_p0>: 'SINGLE',\n", " <CategoricalVariable: bond_type_p1>: 'SINGLE',\n", " <CategoricalVariable: bond_type_p2>: 'SINGLE',\n", " <CategoricalVariable: is_aromatic>: None,\n", " <CategoricalVariable: is_aromatic_p0>: False,\n", " <CategoricalVariable: is_aromatic_p1>: False,\n", " <CategoricalVariable: is_aromatic_p2>: False}"]}, "metadata": {}, "execution_count": 44}]}, {"cell_type": "code", "metadata": {"id": "6-SGotLWj9T1"}, "source": ["def rdmol_to_complete_graph(mol):\n", "  atoms = {rd_atom.GetIdx(): get_atom_features(rd_atom) for rd_atom in mol.GetAtoms()}\n", "  all_pairwise_features = {}\n", "  for atom_a, atom_b in combinations(mol.GetAtoms(), 2):\n", "    all_pairwise_features[frozenset((atom_a.GetIdx(), atom_b.GetIdx()))] = get_pairwise_features(mol, atom_a, atom_b)\n", "  return atoms, all_pairwise_features"], "execution_count": null, "outputs": []}, {"cell_type": "code", "metadata": {"id": "7K9IO5Ljj9T1"}, "source": ["def smiles_to_complete_graph(smiles):\n", "  rd_mol = MolFromSmiles(smiles)\n", "  graph = rdmol_to_complete_graph(rd_mol)\n", "  return graph"], "execution_count": null, "outputs": []}, {"cell_type": "code", "metadata": {"id": "Nafpau7zjqeV"}, "source": ["import multiprocessing\n", "from tqdm.notebook import tqdm\n", "\n", "def process_smiles_record(smiles_record):\n", "  smiles = smiles_record['smiles']\n", "  rdmol = MolFromSmiles(smiles)\n", "  label = smiles_record['label']\n", "  graph = rdmol_to_complete_graph(rdmol)\n", "  return graph, label, smiles_record\n", "\n", "def make_graph_shortest_path_dataset(smiles_records, \n", "                                              atom_features=ATOM_FEATURES, \n", "                                              bond_features=PAIRWISE_FEATURES):\n", "  '''\n", "  Create a new GraphDataset from a list of smiles_records dictionaries.\n", "  These records should contain the key 'smiles' and 'label'. Any other keys will be saved as a 'metadata' record.\n", "  The 'label' record will be ignored, and instead be replaced by the diameter of the graph.\n", "  '''\n", "  graphs = []\n", "  labels = []\n", "  metadata = []\n", "  with multiprocessing.Pool() as pool:\n", "    for graph, label, smiles_record in tqdm(pool.imap(process_smiles_record, smiles_records), total=len(smiles_records)):\n", "      graphs.append(graph)\n", "      labels.append(label)\n", "      metadata.append(smiles_record)\n", "  return GraphDataset(graphs=graphs, \n", "                      labels=labels, \n", "                      node_variables=atom_features, \n", "                      edge_variables=bond_features, \n", "                      metadata=metadata)\n", "  "], "execution_count": null, "outputs": []}, {"cell_type": "code", "metadata": {"colab": {"base_uri": "https://localhost:8080/", "height": 49, "referenced_widgets": ["725536c155514d248be3564b7ed27023", "b429a75a228b44d3b3f546bc096d8b4a", "912e37647c0b4b0a99e82abe56f6870a", "c87efd570f784b8fb4fbebba4d02026e", "9fdcafcc99134f84b770db410746ec86", "4b9c5bf620374fd094547d643fdfb596", "48e163183ee249009a7eea260e980a84", "36334bcfc37645d288d964cbbb9c7e24", "2fc4764867bc46078f342d85cdde95c5", "8251cbbafa524dee8c35bc49eb4a5062", "5e4f8f2f2d0e4d62923b476a639bc3eb"]}, "id": "FyUDrkTRurz5", "executionInfo": {"status": "ok", "timestamp": 1637727354379, "user_tz": -60, "elapsed": 436, "user": {"displayName": "Erik Ylip\u00e4\u00e4", "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Ghv2ZnBVJIOqdwt_HBhCMHTtEbc2FUGzMgmKQEW7VE=s64", "userId": "10747062110949017925"}}, "outputId": "eb8d2f13-4672-444a-cd74-cc51290935a0"}, "source": ["dataset = make_graph_shortest_path_dataset([{'smiles': 'c1ccccc1', 'label':1},{'smiles':'OS(=O)(=O)O', 'label': 0}])"], "execution_count": null, "outputs": [{"output_type": "display_data", "data": {"application/vnd.jupyter.widget-view+json": {"model_id": "725536c155514d248be3564b7ed27023", "version_minor": 0, "version_major": 2}, "text/plain": ["  0%|          | 0/2 [00:00<?, ?it/s]"]}, "metadata": {}}]}, {"cell_type": "code", "metadata": {"colab": {"base_uri": "https://localhost:8080/", "height": 113, "referenced_widgets": ["40707fb8b6c5483daa77dbe8388ea77d", "a4e2b16b93fa4a4480eaa5d82968d4cb", "d7c9560e60524f3eb9d13f507be76845", "ad90db20fb01426a8d7bd75678d70d86", "12399d8665084c0fb823206a270d39e6", "7d6d1f11283c412e875cb5a615fbd71e", "b3ca3b4731874309808164498a016786", "5fded286266c488ca013d0470ef938c0", "a77004a3fc8b46c4a4eb37a66d5b6393", "73722375724d490da0c9204b1cff90b7", "438711dae7b248638552d314e2c17f06", "78b08f8a809941e0a91ecd612a2ad1c7", "5b985f40476a47d4ba27670279bd7c68", "a08d42508c0f472aa24b06397d2b22bc", "b19398937fc940dc9b7c99a60eaa5de3", "513450c477bb4618a1e4551f389811f3", "c2d3d43dd6cf41b0b1dda766e61d2177", "9dccde463a164b8b8dd38bae3d282595", "ff4c332ff04f46199fb43de7b8020e49", "efc815f0efa7432496f5a3f19d6f9d10", "d576ac236be848d9aed8650016e2a38b", "769e82c0ce714e409c6d7495109e9c03", "eec1acab2f8e46e3bdb0fb9a1df7b2f2", "6fc7b0384b224420af2f3f66d32806fb", "25913d636efa426b85425e9418b270a1", "d05946997e6c4ee494aa582cf0228cb1", "1a3b81ff5d76434cb397dc769efd1f5a", "6c4a109f8e9d456e9288886c74a3fe5a", "9845af57dd9b4324957ebfd1bd10f040", "81080d0ef40c416d99255661837a665b", "879464dc11ec40e1ac842f552f450da0", "df593ece314746858ee4d2dc3786b19d", "1caa7d87195f463d9c88480943946308"]}, "id": "8N1gtWB8z67A", "executionInfo": {"status": "ok", "timestamp": 1637727762523, "user_tz": -60, "elapsed": 404487, "user": {"displayName": "Erik Ylip\u00e4\u00e4", "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Ghv2ZnBVJIOqdwt_HBhCMHTtEbc2FUGzMgmKQEW7VE=s64", "userId": "10747062110949017925"}}, "outputId": "f2e94da9-d9f7-4446-c60f-3218d6eadf03"}, "source": ["training_shortest_path_dataset = make_graph_shortest_path_dataset(training_smiles_records)\n", "dev_shortest_path_dataset = make_graph_shortest_path_dataset(dev_smiles_records)\n", "test_shortest_path_dataset = make_graph_shortest_path_dataset(test_smiles_records)"], "execution_count": null, "outputs": [{"output_type": "display_data", "data": {"application/vnd.jupyter.widget-view+json": {"model_id": "40707fb8b6c5483daa77dbe8388ea77d", "version_minor": 0, "version_major": 2}, "text/plain": ["  0%|          | 0/1631 [00:00<?, ?it/s]"]}, "metadata": {}}, {"output_type": "display_data", "data": {"application/vnd.jupyter.widget-view+json": {"model_id": "78b08f8a809941e0a91ecd612a2ad1c7", "version_minor": 0, "version_major": 2}, "text/plain": ["  0%|          | 0/203 [00:00<?, ?it/s]"]}, "metadata": {}}, {"output_type": "display_data", "data": {"application/vnd.jupyter.widget-view+json": {"model_id": "eec1acab2f8e46e3bdb0fb9a1df7b2f2", "version_minor": 0, "version_major": 2}, "text/plain": ["  0%|          | 0/205 [00:00<?, ?it/s]"]}, "metadata": {}}]}, {"cell_type": "code", "metadata": {"id": "E-zEuTUYWNlF"}, "source": ["from torch.utils.data import DataLoader\n", "\n", "batch_size=32\n", "num_dataloader_workers=2 # The colab instances are very limited in number of cpus\n", "\n", "training_shortest_path_dataloader = DataLoader(training_shortest_path_dataset, \n", "                                          batch_size=batch_size, \n", "                                          shuffle=True, \n", "                                          num_workers=num_dataloader_workers, \n", "                                          collate_fn=collate_graph_batch)\n", "dev_shortest_path_dataloader = DataLoader(dev_shortest_path_dataset, \n", "                                     batch_size=batch_size, \n", "                                     shuffle=False, \n", "                                     num_workers=num_dataloader_workers, \n", "                                     drop_last=False, \n", "                                     collate_fn=collate_graph_batch)\n", "test_shortest_path_dataloader = DataLoader(test_shortest_path_dataset, \n", "                                     batch_size=batch_size, \n", "                                     shuffle=False, \n", "                                     num_workers=num_dataloader_workers, \n", "                                     drop_last=False, \n", "                                     collate_fn=collate_graph_batch)"], "execution_count": null, "outputs": []}, {"cell_type": "code", "metadata": {"id": "BEa4yF94VQks"}, "source": ["torch.manual_seed(1729)\n", "d_model = 16\n", "basic_encoder_config = BasicTransformerConfig(d_model=d_model, \n", "                                      n_layers=2, \n", "                                      ffn_dim=16,\n", "                                      head_dim=8,\n", "                                      layer_normalization=True,\n", "                                      dropout_rate=0.1,\n", "                                      residual_connections=True)\n", "basic_transformer = BasicTransformerEncoder(config=basic_encoder_config, \n", "                                            continuous_node_variables=dataset.continuous_node_variables,\n", "                                            categorical_node_variables=dataset.categorical_node_variables,\n", "                                            continuous_edge_variables=dataset.continuous_edge_variables,\n", "                                            categorical_edge_variables=dataset.categorical_edge_variables,\n", "                                            layer_type=EdgeAttributesTransformerLayer)\n", "\n", "head_config = GraphPredictionHeadConfig(d_model=d_model, ffn_dim=32, pooling_type='sum')\n", "prediction_head = GraphPredictionHead(input_dim=d_model, output_dim=1, config=head_config)\n", "\n", "model = GraphPredictionNeuralNetwork(basic_transformer, prediction_head)\n", "\n", "loss_fn = BCEWithLogitsLoss()\n", "trainer = Trainer(model=model, \n", "                  loss_fn=loss_fn, \n", "                  training_dataloader=training_shortest_path_dataloader,\n", "                  dev_dataloader=dev_shortest_path_dataloader)"], "execution_count": null, "outputs": []}, {"cell_type": "markdown", "metadata": {"id": "g14ZBuwGYh3B"}, "source": ["### Slow training\n", "\n", "You will notice that this model is much slower to train than the previous. That's not because of changes to the neural network (you can see that it's exactly the same as before) but because the input is now much more complex. Each edge now has `max_path_length` as many features as before. This makes the dataloading and embedding steps more time consuming.\n", "\n", "In particular, the Colab instances we're using only have 2 processors, so we can't get much help from doing batch preprocessing in parallel. When training on proper hardware, this time would be hidden due to multiprocessing."]}, {"cell_type": "code", "metadata": {"colab": {"base_uri": "https://localhost:8080/", "height": 49, "referenced_widgets": ["c019bd6daa0e431b94132a1347fa7497", "9627888766c34657afa450b28a299a0d", "04b40f9b341a45c19a8381b4f53d44d1", "35ed92a1243f4e5bbe48c543f091a9f9", "874b9c9c4ebc47bb8adc9dbf341cd4e6", "7458e38cdcb1423daa8e12599683ae21", "a7f153f073f0406aadc832d13bcf2113", "31234aed69204fd3b8fbca8bc7cea724", "7c38e527c60f473aa383ff08eedb4316", "b12c43214cd04010ae6b27aaf8b147dd", "5094b650cc534737811d4bd7b5a89846", "49ec3361e813400781d4ad5d24e737ab", "59fd0bc764ac4671920d8577a0f8bb03", "418515455c8f41c89cd8c283a8e76435", "181af8c33d084fc89b55c0c467089042", "97b5e94b42f746f8a1f2f369af536549", "646b4bb55cfe4493911ab2d0b23ba743", "7b569ed2cb7c436a8b9cd17831b373ea", "e8f710881303406088dc63572de16f02", "0a42e60dffa7401dbc19c3db128bcd9e", "af4a17b468074ebeaf363afe252201a3", "084a2cc6eebf4226b68e82cf0c6e2606", "3c0f494b88cb43d8ab0364ae197fa66a", "ad6798e28d9f4892a06ac38a3011378e", "ba52b9a6296f4e3aa857b78aaa638bab", "e390da822da6428c8818919377a4fd82", "b9d11d6a236342aaa7946be490bda837", "91c161ce9cf64d989c57aa2ccfab746c", "d80460e450b941b69ff90c5aeb501294", "1bf0f6422aee4cc6b4472a4079feff0a", "02d7287ea4d74562bf7377f8d76575d6", "ef2c5273f2c54dbc8a871027bf92bee9", "25bc59cf117c4693a939a1a784da4174"]}, "id": "Mco1Fs835iXA", "executionInfo": {"status": "ok", "timestamp": 1637727975341, "user_tz": -60, "elapsed": 201475, "user": {"displayName": "Erik Ylip\u00e4\u00e4", "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Ghv2ZnBVJIOqdwt_HBhCMHTtEbc2FUGzMgmKQEW7VE=s64", "userId": "10747062110949017925"}}, "outputId": "85132df1-6019-4f29-a8b5-f35bd593f10a"}, "source": ["trainer.train(1)"], "execution_count": null, "outputs": [{"output_type": "display_data", "data": {"application/vnd.jupyter.widget-view+json": {"model_id": "c019bd6daa0e431b94132a1347fa7497", "version_minor": 0, "version_major": 2}, "text/plain": ["Epoch:   0%|          | 0/1 [00:00<?, ?it/s]"]}, "metadata": {}}, {"output_type": "display_data", "data": {"application/vnd.jupyter.widget-view+json": {"model_id": "49ec3361e813400781d4ad5d24e737ab", "version_minor": 0, "version_major": 2}, "text/plain": ["Training batch:   0%|          | 0/51 [00:00<?, ?it/s]"]}, "metadata": {}}, {"output_type": "display_data", "data": {"application/vnd.jupyter.widget-view+json": {"model_id": "3c0f494b88cb43d8ab0364ae197fa66a", "version_minor": 0, "version_major": 2}, "text/plain": ["Dev batch:   0%|          | 0/7 [00:00<?, ?it/s]"]}, "metadata": {}}]}, {"cell_type": "markdown", "metadata": {"id": "eE18fjb0odNs"}, "source": ["## A note on efficiency\n", "\n", "We've seen how we can effectively extend our graph neural network to not only use the neighbourhood of aggregation, and this allows us to embed a lot of domain knowledge into how we represent our problems.\n", "\n", "The downside is that this method scales poorly with the size of our graphs. Both computation time and memory demand will scale quadratically with the size of the input.\n", "\n", "With a regular GNN, we can make the neighbourhood aggregation _sparse_, and this is what practical GNN frameworks such as PyTorch Geometric do.\n", "\n", "In the field of NLP, much research is dedicated to making the transformer architecture more efficient and we will likely see developments on how this method can be more efficient in the future."]}, {"cell_type": "markdown", "metadata": {"id": "AKokZbeu4iAD"}, "source": ["## Task\n", "Experiment with this Path-augmented Transformer. Can you do better than the GNN from the previous notebook?\n"]}, {"cell_type": "markdown", "metadata": {"id": "ouKhEI9gLr2U"}, "source": ["## Learning outcomes\n", "\n", "In this notebook we took a deep dive into Transformers. As you can see, we've essentially extended the idea of a graph neural network by _computing_ an \"adjacency matrix\" (the attention matrix). The choice of what function to use to compute each element of this matrix is up to us, but for graphs it's reasonable to use any edge information between the pair of nodes.\n", "\n", "We saw how we could also include _any_ pairwise information, and while we used the shortes path between the nodes in this case, for graphs embedded in a euclidean space it could have been the euclidean distance or any other information we might compute from a pair of node features.\n", "\n", "## Important concepts\n", " - Attention\n", " - Function on pair of nodes\n", " - Shortest path"]}, {"cell_type": "markdown", "metadata": {"id": "zL1bvPu6blyC"}, "source": ["## What about this _multi-head_ attention?\n", "\n", "When talking about Transformers, a lot of time is often given to the _multi-head self-attention_. In this notebook we haven't covered that, mostly because the difference between multi-head and single head self-attention is conceptually small, but quite a bit more messy to implement efficiently.\n", "\n", "### What is multi-head self-attention?\n", "\n", "In multi-head, we effectively add _parallel_ networks in the aggregation part. So think of it as having multiple parallel GNN layers, each performing their own MSG and AGG step. In the end, the resulting vectors of the parallel heads are combined (concatenated in the Transformer) to form the new vector for the node.\n", "\n", "This is equivalent to using [\"block-diagonal\"](https://en.wikipedia.org/wiki/Block_matrix#Block_diagonal_matrices) weight matrices (as well as some shuffling around of results) which is an interesting way of using sparse computation for neural networks. "]}]}